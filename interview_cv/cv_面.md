写在前面的话

这个回答的适用对象主要还是本科和硕士。PhD找工作的套路跟硕士还是很不一样的，所以这个回答的经验对于手握几篇一作顶会的PhD大神并没啥参考意义。

我也和我们实验室几个找工作的PhD学长学姐聊过，他们的面试主要是讲自己的research，有的甚至就是去公司给个talk，跟本科硕士的校招流程完全不同。现在也是AI方向PhD的黄金时代，没毕业就被各大公司主动联系，待遇也比我这种硕士高很多很多。



一.  整体建议

一定要找内推。

内推一般有两种，第一种力度比较弱，在公司的内推系统上填一下你的名字，加快一下招聘流程；第二种力度比较强，直接把简历送到部门负责人手上。个人比较建议第二种，会省事很多。

原因如下：

（1）现在做机器学习的人实在太多了，在不找内推的情况下，流程会特别特别慢。即使你的简历比较优秀，也可能淹没在茫茫大海中，不一定能被懂行的人看到。

（2）现在很多公司的笔试其实挺有难度的，就算是大神也有翻车的可能性。

（3）对于大公司而言，即使通过了简历筛选、笔试那一关，你也很难保证你的简历被合适的部门挑中。很可能过关斩将后，发现给你安排的面试官并不是太对口。尤其是深度学习这样比较新的领域，一般部门的面试官多半也是近期自学的，对这个也是一知半解。所以如果是想去BAT这些大公司里面专门做AI的部门，按照正常校招流程走是不合适的，一定要找到那些部门的员工内推。

在我看来，如果是跪在简历筛选、笔试这些上面，连面试官都没见到，就实在太可惜了。为了避免这一点，请认真找内推。最好能联系到你想去的公司部门里的负责人，直接安排面试。



二. 面试经验

面试遇到的题目，可以分为几个大类：

（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功 

虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。

大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free.  并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。

以下是我所遇到的一些需要当场写出完整代码的题目：

/*******************************************************************

经人提醒，意识到直接贴具体的面试原题是非常不好的行为，该部分不在回答里放出来了。

*******************************************************************/

不过这部分有些是LeetCode原题，在这里我简单地举几个例子，附上LeetCode题目链接：

[Maximum Product Subarray](https://link.zhihu.com/?target=https%3A//leetcode.com/problems/maximum-product-subarray/description/)

[Maximal Square](https://link.zhihu.com/?target=https%3A//leetcode.com/problems/maximal-square/description/)

[Subsets](https://link.zhihu.com/?target=https%3A//leetcode.com/problems/subsets/description/)



（2）数学题或者"智力"题。

不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。

下面是我在面试中被问到过的问题：

/*******************************************************************

经人提醒，意识到直接贴具体的面试原题是非常不好的行为，该部分不在回答里放出来了。

*******************************************************************/

这部分有些题也在知乎上被讨论过，这里附上相应的知乎链接

[如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？](https://www.zhihu.com/question/38331955)

[如何理解矩阵的「秩」？](https://www.zhihu.com/question/21605094)

[矩阵低秩的意义?](https://www.zhihu.com/question/28630628)

[如何理解矩阵特征值？](https://www.zhihu.com/question/21874816)

[奇异值的物理意义是什么？](https://www.zhihu.com/question/22237507)

[为什么梯度反方向是函数值下降最快的方向？](https://zhuanlan.zhihu.com/p/24913912)



（3）机器学习基础

这部分建议参考周志华老师的《机器学习》。

下面是我在面试中被问到过的问题：

/*******************************************************************

经人提醒，意识到直接贴具体的面试原题是非常不好的行为，该部分不在回答里放出来了。

*******************************************************************/

列一下考察的知识点，并附上相关的优质知乎讨论。

逻辑回归，SVM，决策树

[逻辑回归和SVM的区别是什么？各适用于解决什么问题？](https://www.zhihu.com/question/24904422)

[Linear SVM 和 LR 有什么异同？](https://www.zhihu.com/question/26768865)

[SVM（支持向量机）属于神经网络范畴吗？](https://www.zhihu.com/question/22290096)

[如何理解决策树的损失函数?](https://www.zhihu.com/question/34075616)

[各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归和最大熵模型。](https://www.zhihu.com/question/26726794)

主成分分析，奇异值分解

[SVD 降维体现在什么地方？](https://www.zhihu.com/question/34143886)

[为什么PCA不被推荐用来避免过拟合？](https://www.zhihu.com/question/47121788)

随机森林，GBDT, 集成学习

[为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839)

[基于树的adaboost和Gradient Tree Boosting区别？](https://www.zhihu.com/question/46784781)

[机器学习算法中GBDT和XGBOOST的区别有哪些？](https://www.zhihu.com/question/41354392)

[为什么在实际的 kaggle 比赛中 gbdt 和 random forest 效果非常好？](https://www.zhihu.com/question/51818176)

过拟合

[机器学习中用来防止过拟合的方法有哪些？](https://www.zhihu.com/question/59201590)

[机器学习中使用「正则化来防止过拟合」到底是一个什么原理？为什么正则化项就可以防止过拟合？](https://www.zhihu.com/question/20700829)



（4）深度学习基础

这部分的准备，我推荐花书（Bengio的Deep learning）和 

[@魏秀参](http://www.zhihu.com/people/b716bc76c2990cd06dae2f9c1f984e6d)

 学长的[《解析卷积神经网络-深度学习实践手册》](https://link.zhihu.com/?target=http%3A//210.28.132.67/weixs/book/CNN_book.html)



/*******************************************************************

经人提醒，意识到直接贴具体的面试原题是非常不好的行为，该部分不在回答里放出来了。

*******************************************************************/

列一下大概的考察点和相关的知乎讨论。

卷积神经网络，循环神经网络，LSTM与GRU，梯度消失与梯度爆炸，激活函数，防止过拟合的方法，dropout，batch normalization，各类经典的网络结构，各类优化方法

[卷积神经网络工作原理直观的解释？](https://www.zhihu.com/question/39022858)

[卷积神经网络的复杂度分析](https://zhuanlan.zhihu.com/p/31575074)

[CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？](https://www.zhihu.com/question/34681168)

[bp算法中为什么会产生梯度消失？](https://www.zhihu.com/question/49812013)

[梯度下降法是万能的模型训练算法吗？](https://www.zhihu.com/question/38677354)

[LSTM如何来避免梯度弥散和梯度爆炸？](https://www.zhihu.com/question/34878706)

[sgd有多种改进的形式(rmsprop,adadelta等),为什么大多数论文中仍然用sgd?](https://www.zhihu.com/question/42115548)

[你有哪些deep learning（rnn、cnn）调参的经验？](https://www.zhihu.com/question/41631631)

[Adam那么棒，为什么还对SGD念念不忘 (1)](https://zhuanlan.zhihu.com/p/32230623)

[Adam那么棒，为什么还对SGD念念不忘 (2)](https://zhuanlan.zhihu.com/p/32262540)

[全连接层的作用是什么？](https://www.zhihu.com/question/41037974)

[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)

[为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？](https://www.zhihu.com/question/43370067)

[Krizhevsky等人是怎么想到在CNN里用Dropout和ReLu的?](https://www.zhihu.com/question/28720729)





（5）科研上的开放性问题

这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition

下面是我在面试中被问到过的问题：

/*******************************************************************

经人提醒，意识到直接贴具体的面试原题是非常不好的行为，该部分不在回答里放出来了。

*******************************************************************/

这部分在知乎上也有很多讨论，不具体列了。



（6） 编程语言、操作系统等方面的一些问题。

C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了

（7）针对简历里项目/论文 / 实习的一些问题。

这部分因人而异，我个人的对大家也没参考价值，也不列了。
