#### 1  交叉熵损失函数推导


#### 2 SVM 损失函数推导



####  3  CV_interviews 补充

**3 代码实现卷积**

[卷积的三种模式full, same, valid以及padding的same, valid](https://zhuanlan.zhihu.com/p/62760780)

f -->s -->v

full模式的意思是，从filter和image刚相交开始做卷积。

same的意思是，当filter的中心(K)与image的边角重合时，开始做卷积运算。

valid的意思，当filter全部在image里面的时候，进行卷积运算。


**5 python ::-1**

[::-1] 顺序相反操作

[-1] 读取倒数第一个元素

[3::-1] 从下标为3（从0开始）的元素开始翻转读取


**13  为什么max pooling 要更常用？什么场景下 average pooling 比 max pooling 更合适**

讲一下 pooling 的作用， 为什么 max pooling 要更常用？哪些情况下，average pooling 比 max pooling 更合适？

作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。

通常来讲，max-pooling 的效果更好，虽然 max-pooling 和 average-pooling 都对数据做了下采样，但是 max-pooling 感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。 pooling 的主要作用一方面是去掉冗余信息，一方面要保留 feature map 的特征信息，在分类问题中，我们需要知道的是这张图像有什么 object，而不大关心这个 object 位置在哪，在这种情况下显然 max pooling 比 average pooling 更合适。在网络比较深的地方，特征已经稀疏了，从一块区域里选出最大的，比起这片区域的平均值来，更能把稀疏的特征传递下去。

average-pooling 更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说 DenseNet 中的模块之间的连接大多采用 average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。

average-pooling 在全局平均池化操作中应用也比较广，在 ResNet 和 Inception 结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替 Flatten 操作，使输入数据变成一位向量。

**16 卷积层相较于全连接层的优势**

卷积层相较于全连接层需要训练的参数更少，所以神经网络的设计离不开卷积层

卷积层通过参数共享和稀疏连接两种方式来保证单层卷积中的训练参数少

**17 网络中常用的损失函数汇总：**

https://zhuanlan.zhihu.com/p/58883095

**18 有哪些修改、调试模型的经验分享**

**19 目标检测评价指标mAP的计算**

见 2020_algorithm_intern_information

**20  实例分割中的评价指标：**

https://blog.csdn.net/weixin_40546602/article/details/105292391 

**21 手动推导反向传播公式BP**

鸡： 计算图

鸡： 激活函数

踢： 梯度下降

连： 链式求导

长：张量

**22 深度分离理解**

[深度可分离卷积](https://zhuanlan.zhihu.com/p/92134485)

深度卷积负责滤波，尺寸为(DK,DK,1)，共M个，作用在输入的每个通道上；逐点卷积负责转换通道，尺寸为(1,1,M)，共N个，作用在深度卷积的输出特征映射上。

**25  batch size 和 learning rate 的关系：** 

https://www.zhihu.com/question/64134994/answer/216895968

通常当我们增加batchsize为原来的N倍时，要保证经过同样的样本后更新的权重相等，按照线性缩放规则，学习率应该增加为原来的N倍[5]。

但是如果要保证权重的方差不变，则学习率应该增加为原来的sqrt(N)倍[7]，目前这两种策略都被研究过，使用前者的明显居多。使用这样的策略，就可以缓解大的batchsize带来的以上问题。

对此实际上是有两个建议：如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。

尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。

**26 类别不均衡问题怎么解决**

https://zhuanlan.zhihu.com/p/56882616

https://blog.csdn.net/vivian_ll/article/details/105201120

**28 权重初始化方法有哪些**

深度学习中神经网络的几种权重初始化方法

Xavier初始化：

Xavier initialization是 Glorot 等人为了解决随机初始化的问题提出来的另一种初始化方法，他们的思想倒也简单，就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。

虽然Xavier initialization能够很好的 tanH 激活函数，但是对于目前神经网络中最常用的ReLU激活函数，还是无能能力。

He初始化：

提出了一种针对ReLU的初始化方法，一般称作 He initialization。

https://zhuanlan.zhihu.com/p/72374385

**29  计算机视觉中的注意力机制**

https://zhuanlan.zhihu.com/p/146130215

**30  onehot 编码可能会遇到的问题**

onehot适用：使用one-hot编码来处理离散型的数据，使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。

one-hot编码要求每个类别之间相互独立，如果之间存在某种连续型的关系，或许使用distributed respresentation（分布式）更加合适。

One-hot encoding

优点：解决了分类器不好处理分类数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。

缺点：当类别的数量很多时，特征空间会变得非常大，容易造成维度灾难。

Label encoding

优点：解决了分类编码的问题，可以自由定义量化数字。但其实也是缺点，因为数值本身没有任何含义，只是排序。如大中小编码为123，也可以编码为321，即数值没有意义。

缺点：可解释性比较差。比如有[dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]，这里就产生了一个奇怪的现象：dog和mouse的平均值是cat。因此，Label encoding编码其实并没有很宽的应用场景。

定类类型的数据，建议使用one-hot encoding，是纯分类，不排序。定序类型的数据，建议使用label encoding。

使用稀疏向量来节省空间，目前大部分算法支持稀疏向量形式的输入。

连续型数据预处理：二值化 和 分段。

**31  分类问题有哪些评价指标？每种的适用场景**


**32  L1 正则化与 L2 正则化的区别**

https://cloud.tencent.com/developer/article/1456966

**35  resnet 流行的原因**

从上面两个图可以看出，在网络很深的时候（56 层相比 20 层），模型效果却越来越差了（误差率越高），并不是网络越深越好。

ResNet 创造性的引入了残差单元，很好的解决了这个问题：

    引入残差单元，简化学习目标和难度，加快训练速度，模型加深时，不会产生退化问题

    引入残差单元，能够有效解决训练过程中梯度消失和梯度爆炸问题


ResNet 第二个版本做了哪些改进？

ResNet_v2 与 v1 的最大区别就是 v2 的 BN 和 ReLU 是在卷积之前使用的，好处：

     反向传播基本符合假设，信息传递无阻碍；
 
     BN 层作为 pre-activation，起到了正则化的作用；

**36 mobileNet、shuffleNet **

MobileNet 是为移动和嵌入式设备提出的高效模型。MobileNets 基于流线型架构 (streamlined)，使用深度可分离卷积 (即 Xception 变体结构) 来构建轻量级深度神经网络。宽度因子 α 用于控制输入和输出的通道数，分辨率因子 ρ 控制输入的分辨率。

例如，对于深度分离卷积，把标准卷积 (4,4,3,5) 分解为：

深度卷积部分：大小为 (4,4,1,3)，作用在输入的每个通道上，输出特征映射为 (3,3,3)

逐点卷积部分：大小为 (1,1,3,5)，作用在深度卷积的输出特征映射上，得到最终输出为 (3,3,5)

shuffleNet 专门应用于计算力受限的移动设备，主要包含 2 个操作： 逐点群卷积（降低计算复杂度）和通道混洗（帮助信息流通）

**38 了解全卷积网络FCN 和 UNet**

F --> add, U --> concat

FCN： ⽹络结构是：下采样（卷积＋池化，使⽤的是VGG）——反卷积（反卷积之前还有⼀个跳跃 链接以利⽤不同层的信息的操作）

⼀种是 ResNet 和 FPN 等当中采⽤的 element-wise add ，另⼀种 是 DenseNet 等中采⽤的 concat

**40 ROI pooling和ROI align的区别**

ROI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作

ROI Pooling和ROIAlign最⼤的区别是：前者使⽤了两次量化操作，⽽后者并没有采⽤量化操作，使⽤了双线性插值算法。

双线性差值：不需要进行取整操作，如果计算得到小数，也就是没有落到真实的pixel上，那么就用最近的pixel对这一点虚拟pixel进行双线性插值，得到这个“pixel”的值。

featuremap等分--- > 落点区域邻近4点插值--- > 求max。

**42 介绍下FPN网络**

利用不同stage的特征图，形成特征金字塔网络（feature parymid network），表征不同scale的物体，然后再基于特征金字塔做物体检测，也就是进入了FPN时代。

**45 卷积网络压缩方法**

剪枝：

首先将低于某个阈值的权重连接全部剪除，之后对剪枝后的网络进行微调以完成参数更新的方法

利用稀疏约束来对网络进行剪枝也是一个研究方向，其思路是在网络的优化目标中加入权重的稀疏正则项，使得训练时网络的部分权重趋向于 0 ，而这些 0 值就是剪枝的对象。

先剪枝 - 再fintune revocer -再network prune

原因：小的network难训

参数量化：

所谓“量化”，是指从权重中归纳出若干“代表”，由这些“代表”来表示某一类权重的具体数值。“代表”被存储在码本（codebook）之中，而原权重矩阵只需记录各自“代表”的索引即可，从而极大地降低了存储开销。

标量量化：转向量，做聚类，聚类中心存储在码本


知识蒸馏：

学生预测结果尽量接近老师预测结果， 能和老师学到比label更多的东西，可以ensemble net 让多个老师来教学生

Temperature： 因为使用了指数函数，如果在使用softmax之前的预测值是x1=100,x2=10,x3=1,那么使用softmax之后三者对应的概率接近于y1=1,y2=0,y3=0，那这和常规的label无异了，

所以为了解决这个问题就引入了一个新的参数T,称之为Temperature

**46 LSTM**

LSTM是在RNN的基础上改进而来的，解决的是长序列数据训练过程中的梯度消失和梯度爆炸问题

LSTM结构相对于普通的RNN多了一个传输状态 (cell state)

**49 maskrcnn**

整个Mask R-CNN算法的思路很简单，就是在原始Faster-rcnn算法的基础上⾯增加了FCN来产⽣对应的MASK分⽀。即Faster-rcnn + FCN，更细致的是 RPN + ROIAlign + Fast-rcnn + FCN。

首先对输入的图片进行裁剪操作，并将裁剪后的图片送入预训练好的分类网络中获取该图像对应的特征图；然后在特征图上的每一个锚点上取9个候选的ROI（3个不同尺度，3个不同⻓宽⽐），并根据相应的比例将其映射到原始图像中（因为特征提取网络一般有conv和pool组成，但是只有pool会改变特征图的大小，因此最终的特征图大小和pool的个数相关）；

接着将这些候选的ROI输入到RPN网络中，RPN网络对这些ROI进行分类（即确定这些ROI是前景还是背景）同时对其进行初步回归（即计算这些前景ROI与真实目标之间的BB的偏差值，包括Δx、Δy、Δw、Δh）,然后做NMS（非极大值抑制，即根据分类的得分对这些ROI进行排序，然后选择其中的前N个ROI）；

接着对这些不同大小的ROI进行ROI Pooling操作（即将其映射为特定大小的feature_map,文中是7x7），输出固定大小的feature_map；

最后将其输入简单的检测网络中，然后利用1x1的卷积进行分类（区分不同的类别，N+1类，多余的一类是背景，用于删除不准确的ROI），同时进行BB回归（精确的调整预测的ROI和GT的ROI之间的偏差
值），从而输出一个BB集.

Lmask loss: 这与利用FCN进行语义分割的有所不同，它通常使一个per-pixel sigmoid和一个multinomial cross-entropy loss ，在这种情况下mask之间存在竞争关系；而由于我们使用了一个per-pixel sigmoid 和一个binary loss ，不同mask之间不存在竞争关系。

RPN的分类损失是二分类的交叉熵损失，fast rcnn 是多分类的交叉熵损失。

RPN损失：分类+回归。   fast rcnn损失：分类+回归。

**43  one stage目标检测网络 YOLO 和 SSD 网络结构及优缺点**

[深入理解one-stage目标检测方法](https://zhuanlan.zhihu.com/p/61485202)

**50 yolov3 v4  v5对比总结**

[深入浅出Yolo系列之Yolov3&Yolov4&Yolov5核心基础知识完整讲解](https://zhuanlan.zhihu.com/p/143747206)

[深入浅出Yolo系列之Yolov5核心基础知识完整讲解](https://zhuanlan.zhihu.com/p/172121380)

**51 深度学习模型设计与训练tricks**

[cnn结构设计技巧-兼顾速度精度与工程实现（持续更新）](https://zhuanlan.zhihu.com/p/100609339)

[本科生晋升GM记录 & kaggle比赛进阶技巧分享](https://zhuanlan.zhihu.com/p/93806755)




