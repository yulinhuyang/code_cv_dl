random Forest 


随机选择样本（放回抽样） ---> 随机选择特征  ---> 构建决策树  ---> 随机森林投票（平均）

随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；随机选择特征是指在每个节点在分裂过程中都是随机选择特征的(K个属性的子集里面选择最优)

GBDT

Regression Decision Tree（即 DT）---> Gradient Boosting（即 GB）---> Shrinkage（一个重要演变）

DT: GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是回归树。

GB:GBDT 的每一棵树都是以之前树得到的残差来更新目标值，每一棵树的值加起来即为 GBDT 的预测值。每次只训练一个基模型。

    一般回归类的损失函数会用绝对损失或者 Huber 损失函数来代替平方损失函数

Shrinkage:每棵树设置了一个 weight，累加时要乘以这个 weight

Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）



25  线性回归与L1、L2相关

https://zhuanlan.zhihu.com/p/74874291

Logistic Regression 虽然被称为回归，但其实际上是分类模型。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计

先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。

L1 正则化：LASSO 回归，增加先验知识：w 服从零均值拉普拉斯分布，参数稀疏，完成特征自动选择

L2 正则化：Ridge 回归，w 服从零均值正态分布。权重衰减，不容易交在坐标轴上，但是仍然比较靠近坐标轴
