
LR ->SVM ->Xgboost 

LR——>最大熵模型

EM-->GMM


### 30 聚类算法 

算法：5+1

算法：

K-Means：

Mean-Shift：目标是定位每个簇/类的质心，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件（找出最密集的区域）

     画圈——>圆心指向质心移动->重复

DBSCAN：数据点为圆心，eps半径画圆-> 阈值分点，低密度点和高密度点-> 合并相邻高密度点

EM聚类：GMM聚类

	初始化GMM参数 -->对于每个样本：固定各个高斯分布，计算样本在各个高斯分布上的概率

	-->固定样本的生成概率，更新分布参数，使聚类内数据点的概率最大化 --> 迭代

层次聚类：凝聚法和分裂法，阈值决定分裂合并

谱聚类：


评价指标：https://blog.csdn.net/liuy9803/article/details/80762862

性能度量（相似度系数）和距离计算


### 44 PCA 

最大可分性（最大化投影方差，协方差最大特征值）、最小重构性（寻找超平面使样本点到超平面距离平方和最小）

样本中心化，零均值->求协方差矩阵的特征值和特征向量->特征值排列，取前K个特征向量-> N维降维k维

### 45 LDA 

设法将样本投影到一条直线上，最小化类内距离，最大化类间距离

计算样本均值向量 -> 计算类间散度和类内散度 -> 类间逆乘类内特征值对应特征向量排序 ——> 取前K个特征向量 ——> N维度降K维

LDA 线性有监督的降维方法；对噪声鲁棒，模型简单表达能力不足；假设类别都是高斯分布、各类别协方差相等

LDA选择分类性能最好的投影方向性，PCA选择样本点投影具有最大方差的方向。

### 49 判别模型与生成模型

判别式模型：条件概率分布建模

生成模型：HMM GMM 朴素贝叶斯  LDA，联合概率分布建模

监督算法：

无监督算法：


### 50_57 如何评价分类和回归算法模型的效果

评价指标：(周志华《Machine Learning》学习笔记(2)--性能度量.md)

分类：精确率、召回率、准确率、F值、ROC-AUC 、混淆矩阵、PRC

回归：RMSE(平方根误差)、MAE（平均绝对误差）、MSE(平均平方误差)

聚类：兰德指数、互信息、轮廓系数

损失函数：参见 57


### 53 如何进行特征选择 

https://www.zhihu.com/question/28641663

周志华《Machine Learning》学习笔记(13)--特征选择与稀疏学习.md

1+3: 预处理+ Filter、Wrapper、Embedded

**预处理**

标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。

归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”

无量纲化、区间缩放、缺失值

**Filter**

过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

方差选择：选择方差大于阈值的特征

相关系数法：先要计算各个特征对目标值的相关系数以及相关系数的P值

卡方检验：定性自变量对定性因变量的相关性

互信息法：评价定性自变量对定性因变量的相关性的，最大信息系数法

**Wrapper**

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练

**Embedded**

使用带惩罚项的基模型(L1/L2惩罚项)，除了筛选出特征外，同时也进行了降维

**57 损失函数**

回归：

	MSE: 当模型输出与真实值的误差服从高斯分布的假设，最小化均方差损失函数与极大似然估计本质上是一样的，平方和。

	MAE: 当模型预测值与真实值之间的误差服从拉普拉斯分布，绝对值。

		MSE比MAE通常可以更快的收敛，MAE比MSE更加健壮。

	Huber loss : MAE + MSE  + 超参数,误差小使用MSE,误差大使用MAE
		
	quantile loss(分位数损失)：用不同的系数控制高估和低估的损失，进而实现分位数回归。

分类：

	交叉熵损失：二分类、多分类
	
	合页损失： SVM损失函数为合页损失 + L2正则化，max（0,1-y*y'）
	
	指数损失；exp(-y*f(x))  adaboost的损失函数

	
**59 线性回归的平方形式**

最小化平方误差的本质等同于在误差服从高斯分布的假设下的最大似然估计。
	
在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的。

所以误差服从高斯分布的情况下，所以线性回归的最小二乘法和逻辑回归的极大似然估计本质上是一样的。
		
	
