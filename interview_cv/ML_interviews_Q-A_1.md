
李宏毅笔记： https://github.com/Sakura-gh/ML-notes

李航笔记： https://github.com/SmirkCao/Lihang


### 01 LR和SVM的比较 

理解svm 和hinge loss 

https://zhuanlan.zhihu.com/p/49331510

https://www.zhihu.com/question/47746939


KKT条件。(1)是对拉格朗日函数取极值时候带来的一个必要条件，(2)是拉格朗日系数约束（同等式情况），(3)是不等式约束情况，(4)是互补松弛条件

### 09  SVM与SMO算法 

[【机器学习】支持向量机 SVM（非常详细）](https://zhuanlan.zhihu.com/p/77750026)


### 10 集成学习相关问题 

Bagging和Boosting 的主要区别： 放回、权值、并串行

样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是买个样的权重。

样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大。

预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。

并行计算: Bagging 的各个预测函数可以并行生成; Boosting的各个预测函数必须按照顺序迭代串行生成。

将决策树与以上框架组合成新的算法

Bagging + 决策树 = 随机森林

AdaBoost + 决策树 = 提升树

gradient + 决策树 = GDBT

boosting:  Improving Weak Classifiers 


### 11 集成学习算法总结 

[【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）](https://zhuanlan.zhihu.com/p/86263786)

**AdaBoost**

Adaptive Boosting

训练过程：改变样本的权值，改变分类器的系数。

分类错误率越小的样本，在最终分类器中作用越大。

基本分类器误分类的权值增大，正确分类的权值减小。

AdaBoost 算法是模型为加法模型、损失函数为指数函数

**GBDT**

Gradient Boosting Decision Tree

adaboost模型每个基分类器的损失函数优化目标都是相同的且独立的，都是优化当前样本的指数损失。

GBDT虽然也是一个加性模型，但其是通过不断迭代拟合样本真实值与当前分类器预测的残差来逼近真实值的。

GBDT无论是用于分类和回归，采用的都是回归树，分类问题最终是将拟合值转换为概率来进行分类的。

Gradient Boosting，其思想类似于数值优化中梯度下降求参方法，参数沿着梯度的负方向以小步长前进，最终逐步逼近参数的局部最优解。在GB中模型每次拟合残差，逐步逼近最终结果。


### 12 XGboost总结

[【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678)

[xgboost面试题总结](https://blog.csdn.net/weixin_38753230/article/details/100571499)


损偏  复杂方

损失函数+ 正则化复杂度

xgboost extreme gradient tree        极端梯度提升树


四大特征：目标函数、最优切分点划分算法、加权分位数缩略图、稀疏感知算法

目标函数

	每一步损失函数的一阶导和二阶导的值，加入正则化项

	正则化：决策树模型的复杂度由生成的所有决策树的叶子节点数量，和所有节点权重所组成的向量的L2范式决定

最优切分点划分算法

	最优切分点划分算法：支持两种分裂节点的方法——贪心算法和近似算法

	贪心算法：对每个叶节点枚举所有的可用特征。针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；

	近似算法（Global、Local）：分位点，该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。

加权分位数缩略图：
	
	XGboost按照样本个数的二阶导数值作为样本权值进行划分
	
稀疏感知算法：
    
    XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向。
	
	  如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。
	
三大优化：块结构设计、缓存访问优化算法、“核外”块计算

块结构设计

	决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序。
	
	XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了
	
	稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构
	
	每一个块结构包括一个或多个已经排序好的特征；每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；

缓存访问优化算法
	
	XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换


“核外”块计算
    
	当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。
	
XGBoost 还用了两种方法来降低硬盘读写的开销：块压缩：对 Block 进行按列压缩，并在读取时进行解压；块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。



### 13  LightGBM

XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 

数学原理：单边梯度抽样算法、直方图算法、互斥特征捆绑算法、

单边梯度抽样算法:GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样,在计算增益时为梯度小的样本引入一个常数进行平衡

直方图算法: 离散化K(8位去存储直方图) --> 直方图加速(通过父节点的直方图与相邻叶节点的直方图相减)--> 稀疏特征优化(只用非零特征构建直方图)

互斥特征捆绑算法、如果将一些特征进行融合绑定，则可以降低特征数量。


#### 15 机器学习其他补充  LR  --> SVM  --> xgboost

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

[【机器学习】降维——PCA（非常详细）](https://zhuanlan.zhihu.com/p/77151308)



