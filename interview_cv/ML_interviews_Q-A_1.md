
李宏毅笔记： https://github.com/Sakura-gh/ML-notes

李航笔记： https://github.com/SmirkCao/Lihang


### 01 LR和SVM的比较 

理解svm 和hinge loss 

https://zhuanlan.zhihu.com/p/49331510

https://www.zhihu.com/question/47746939


KKT条件。(1)是对拉格朗日函数取极值时候带来的一个必要条件，(2)是拉格朗日系数约束（同等式情况），(3)是不等式约束情况，(4)是互补松弛条件

### 09  SVM与SMO算法 

[【机器学习】支持向量机 SVM（非常详细）](https://zhuanlan.zhihu.com/p/77750026)


### 10 集成学习相关问题 

Bagging和Boosting 的主要区别： 放回、权值、并串行

样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是买个样的权重。

样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大。

预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。

并行计算: Bagging 的各个预测函数可以并行生成; Boosting的各个预测函数必须按照顺序迭代串行生成。

将决策树与以上框架组合成新的算法

Bagging + 决策树 = 随机森林

AdaBoost + 决策树 = 提升树

gradient + 决策树 = GDBT

boosting:  Improving Weak Classifiers 


### 11 集成学习算法总结 

[【机器学习】决策树（中）——Random Forest、Adaboost、GBDT （非常详细）](https://zhuanlan.zhihu.com/p/86263786)

**random Forest**


随机选择样本（放回抽样） ---> 随机选择特征  ---> 构建决策树  ---> 随机森林投票（平均）

随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；随机选择特征是指在每个节点在分裂过程中都是随机选择特征的(K个属性的子集里面选择最优)


**AdaBoost**

Adaptive Boosting

训练过程：改变样本的权值，改变分类器的系数。

分类错误率越小的样本，在最终分类器中作用越大。

基本分类器误分类的权值增大，正确分类的权值减小。

AdaBoost 算法是模型为加法模型、损失函数为指数函数

**GBDT**

Gradient Boosting Decision Tree


Regression Decision Tree（即 DT）---> Gradient Boosting（即 GB）---> Shrinkage（一个重要演变）

DT: GBDT 的核心在于累加所有树的结果作为最终结果，所以 GBDT 中的树都是回归树。

GB:GBDT 的每一棵树都是以之前树得到的残差来更新目标值，每一棵树的值加起来即为 GBDT 的预测值。每次只训练一个基模型。

    一般回归类的损失函数会用绝对损失或者 Huber 损失函数来代替平方损失函数

Shrinkage:每棵树设置了一个 weight，累加时要乘以这个 weight

Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）


adaboost模型每个基分类器的损失函数优化目标都是相同的且独立的，都是优化当前样本的指数损失。

GBDT虽然也是一个加性模型，但其是通过不断迭代拟合样本真实值与当前分类器预测的残差来逼近真实值的。

GBDT无论是用于分类和回归，采用的都是回归树，分类问题最终是将拟合值转换为概率来进行分类的。

Gradient Boosting，其思想类似于数值优化中梯度下降求参方法，参数沿着梯度的负方向以小步长前进，最终逐步逼近参数的局部最优解。在GB中模型每次拟合残差，逐步逼近最终结果。




### 12 XGboost总结

[【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678)

[xgboost面试题总结](https://blog.csdn.net/weixin_38753230/article/details/100571499)


损偏  复杂方

损失函数+ 正则化复杂度

xgboost extreme gradient tree        极端梯度提升树


四大特征：目标函数、最优切分点划分算法、加权分位数缩略图、稀疏感知算法

目标函数

	每一步损失函数的一阶导和二阶导的值，加入正则化项

	正则化：决策树模型的复杂度由生成的所有决策树的叶子节点数量，和所有节点权重所组成的向量的L2范式决定

最优切分点划分算法

	最优切分点划分算法：支持两种分裂节点的方法——贪心算法和近似算法

	贪心算法：对每个叶节点枚举所有的可用特征。针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；

	近似算法（Global、Local）：分位点，该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。

加权分位数缩略图：
	
	XGboost按照样本个数的二阶导数值作为样本权值进行划分
	
稀疏感知算法：
    
    XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向。
	
	  如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。
	
三大优化：块结构设计、缓存访问优化算法、“核外”块计算

块结构设计

	决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是都需要对特征的值进行排序。
	
	XGBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了
	
	稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构
	
	每一个块结构包括一个或多个已经排序好的特征；每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；

缓存访问优化算法
	
	XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换


“核外”块计算
    
	当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据暂存到硬盘中，XGBoost 独立一个线程专门用于从硬盘读入数据，以实现处理数据和读入数据同时进行。
	
XGBoost 还用了两种方法来降低硬盘读写的开销：块压缩：对 Block 进行按列压缩，并在读取时进行解压；块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。



### 13  LightGBM

XGBoost_hist（利用梯度直方图的 XGBoost） 和 LightGBM 

数学原理：单边梯度抽样算法、直方图算法、互斥特征捆绑算法、带深度限制的Leaf-wise 算法、类别特征最优分割

单边梯度抽样算法:

	GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样,在计算增益时为梯度小的样本引入一个常数进行平衡

直方图算法: 

	离散化K(8位去存储直方图) --> 直方图加速(通过父节点的直方图与相邻叶节点的直方图相减)--> 稀疏特征优化(只用非零特征构建直方图)

互斥特征捆绑算法：

	如果将一些特征进行融合绑定，则可以降低特征数量。


带深度限制的Leaf-wise 算法

	在建树的过程中有两种策略：

		Level-wise：基于层进行生长，直到达到停止条件；

		Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件。

		XGBoost 采用 Level-wise 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 Leaf-wise 的增长策略减少了计算量.
		
	
类别特征最优分割

	one-hot 编码问题：样本切分不平衡问题，切分增益会非常小。影响决策树学习。
	
	每次分组时都会根据训练目标对类别特征进行分类。
	
工程实现: 特征并行、数据并行、投票并行、 缓存优化


特征并行：每台机器都有训练集完整数据，在得到最佳划分方案后可在本地执行划分

数据并行：分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信

投票并行：通过投票的方式只合并部分特征的直方图从而达到降低通信量的目的

缓存优化：只需要对梯度进行排序并可实现连续访问

#### 25  线性回归与L1、L2相关

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

Logistic Regression 虽然被称为回归，但其实际上是分类模型。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计

先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。

L1 正则化：LASSO 回归，增加先验知识：w 服从零均值拉普拉斯分布，参数稀疏，完成特征自动选择

L2 正则化：Ridge 回归，w 服从零均值正态分布。权重衰减，不容易交在坐标轴上，但是仍然比较靠近坐标轴

与线性回归：逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法

和最大熵模型：逻辑回归和最大熵模型本质上没有区别，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。

与SVM：LR 是一个统计的方法，SVM 是一个几何的方法。LR 是参数模型，SVM 是非参数模型。


与朴素贝叶斯： 逻辑回归是判别式模型 p(y|x)，朴素贝叶斯是生成式模型 p(x,y)。判别式模型估计的是条件概率分布，生成式模型估计的是联合概率分布。



#### 26-29   k-means 

随机选择K中心-->计算各样本到中心距离，分配最近-->更新中心 -->迭代

k-means++：已经选取了n个初始聚类中心（0<n<k），那么在选择第n+1个聚类中心时，距离当前已有的n个聚类中心越远的点越有可能被选取为第n+1个聚类中心

K值：手肘法、Gap Statistic 方法

**ISODATA法：**

分离--合并

当某个类别样本数目过多、分散程度较大时，将该类别分为两个子类别。（分裂操作，即增加聚类中心数）

当属于某个类别的样本数目过少时，把该类别去除掉。（合并操作，即减少聚类中⼼数）

**与GMM区别**

初始化GMM参数 -->对于每个样本：固定各个高斯分布，计算样本在各个高斯分布上的概率

-->固定样本的生成概率，更新分布参数 --> 迭代

EM 算法计算GMM参数


k-means算法是非概率模型，而GMM是概率模型
 
k-means计算的是簇类的均值，GMM计算的是高斯分布的参数
 
k-means是硬聚类，而GMM算法是软聚类

 
### 30  聚类方法

[数据科学家需要了解的5种聚类算法](https://zhuanlan.zhihu.com/p/37381630)


### 34 基本树补充

剪枝策略：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。




