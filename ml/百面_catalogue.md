# **1　特征工程**

## 1.1　特征归一化

线性函数（min-max）归一化：0-1

零均值归一化

决策树节点分裂主要依据信息增益比，是否归一化影响不大。

## 1.2　类别型特征

序号编码

独热编码：稀疏向量，配合特征选择来降低维度。

二进制编码

## 1.3　高维组合特征的处理

## 1.4　组合特征

## 1.5　文本表示模型

词袋模型

TF-IDF：单词t在文档中出现到频率  乘以逆文档频率（单词t对表达语义的重要性）

N-gram：n个词组成词组作为一个单独的特征放到向量表中去。

主题模型：主题分布。

词嵌入：词向量化。

## 1.6　Word2Vec

CBOW:.根据上下文词语，预测当前词生成概率。

skip-gram:根据当前词预测上下文各词生成概率。

LDA：隐狄利克雷，利用文档中单词的共现关系进行聚类，概率图模型。

## 1.7　图像数据不足时的处理方法

简化模型

数据扩充

数据增强：变换 增强 加噪声 颜色变换（PCA）

SMOTE：特征提取后，特征空间变换

# **2　模型评估**

2.1　评估指标的局限性

精确率和召回率的平衡

PR曲线：横轴精确率，纵轴召回率。

一个点代表某个阈值下，大于阈值的为正样本，小于的为负样本。

绘制：整个PR曲线是通过将阈值从高到低移动生成。

F1 score精确率和召回率的调和平均。

回归模型评价：RMSE，离群点存在问题。MAPE：更鲁棒指标。


## 2.2　ROC 曲线

受试者工作特征曲线

横坐标：假阳FPR 纵坐标：TPR真阳 

绘制：动态的调整截断点（区分正负预测结果的阈值）每个截断点对应一个FPR TPR

简单方法：横 1/N 纵 1/P，零点开始，正样本沿纵轴绘一个刻度，负样本沿横轴一个刻度。

AUC计算：0.5-1之间，越大越好

PR和ROC对比：正负样本分布变化时，ROC形状基本不变，PR剧烈变化。

ROC更稳定反应模型好坏。


## 2.3　余弦距离的应用

两个向量相似度为余弦相似度｛-1，1｝，1-相似度为余弦距离，｛0，2｝

向量维度高时候，可以保持特性

欧式距离体现数值上差异，

余弦距离体现方向差异。

特性：正定性 对称性

三角不等式：不满足

KL：不满足对称性和三角不等式。


## 2.4　A/B 测试的陷阱

在线A/B测试必要性：

离线测试不会考虑线上的延迟 缺失标签等情况。

离线：关注ROC PR

在线：用户点击率 留存时长 PV访问量等。

方法：用户分桶，实验组和对照组。

划分方法：


## 2.5　模型评估的方法

Holdout检验：随机划分

交叉检验：k-fold交叉验证 留一验证

自助法：自助采样法，总数为n，有放回抽取n次。

自助法，n趋于无穷大，有36.8数据未被采样过。


## 2.6　超参数调优

网格搜索：先使用较宽范围和较大步长，逐渐缩小搜索范围。

随机搜索：不再测试上界和下界。

贝叶斯优化：先根据先验分布，假设一个搜索函数，然后使用新采样点测试目标函数，再更新先验分布。探索和利用的平衡。


## 2.7　过拟合与欠拟合

降低过拟合风险：更多训练数据 降低复杂度 正则化模型参数 集成学习

降低欠拟合风险：添加新特征（因子分解机 梯度提升决策树） 增加模型复杂度 减小正则化系数


# **3　经典算法**

## 3.1　支持向量机

## 3.2　逻辑回归

## 3.3　决策树

**4　降维**

4.1　PCA 最大方差理论

4.2  PCA 最小平方误差理论

4.3　线性判别分析

4.4　线性判别分析与主成分分析

5　非监督学习**

**5.1　K均值聚类

5.2　高斯混合模型

5.3　自组织映射神经网络

5.4　非监督学习算法的评估

**6　概率图模型**

6.1　概率图模型的联合概率分布

6.2　概率图表示

6.3　生成式模型与判别式模型

6.4　马尔可夫模型

6.5　主题模型

**7　优化算法**

7.1　有监督学习的损失函数

7.2　机器学习中的优化问题

7.3　经典优化算法

7.4　梯度验证

7.5　随机梯度下降法

7.6　随机梯度下降法的加速

7.7　L1 正则化与稀疏性

**8　采样**

8.1　采样的作用

8.2　均匀分布随机数

8.3　常见的采样方法

8.4　高斯分布的采样

8.5　马尔科夫蒙特卡洛采样法

8.6　贝叶斯网络的采样

8.7　不均衡样本集的重采样

**9　前向神经网络**

9.1　多层感知机与布尔函数

9.2　深度神经网络中的激活函数

9.3　多层感知机的反向传播算法

9.4　神经网络训练技巧

9.5　深度卷积神经网络

9.6　深度残差网络

**10　循环神经网络**

10.1　循环神经网络和卷积神经网络

10.2　循环神经网络的梯度消失问题

10.3　循环神经网络中的激活函数

10.4　长短期记忆网络

10.5　Seq2Seq 模型

10.6　注意力机制

**11　强化学习**

11.1　强化学习基础

11.2　视频游戏里的强化学习

11.3　策略梯度

11.4　探索与利用

**12　集成学习**

12.1　集成学习的种类

12.2　集成学习的步骤和例子

12.3　基分类器

12.4　偏差与方差

12.5　梯度提升决策树的基本原理

GBDT：根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器累加结合到现在模型中。

GBDT：通常使用CART。梯度提升训练。

对比梯度下降和提升：都是在一轮迭代中，利用损失函数相对模型的负梯度方向的信息来对当前模型进行更新。梯度下降：模型参数化形式表示，模型更新等价于参数更新。梯度提升：模型不参数化表示，直接定义在函数空间中。大大扩展了可以使用模型种类。

优点：快，表达泛化好，可解释。

缺点：高维稀疏数据或文本分类特征，优势不明显。需要串行，需要局部并行提高速度。


12.6　XGBoost与GBDT 的联系和区别

XGBoost:决策树构建阶段加入正则项。

GBDT：基于负梯度构建新树，完成后剪枝。

XGBoost：最大化分裂前后损失函数差值进行决策树构建。遍历所有特征的所有取值。

总结：

（1）xGBoost是GBDT工程实现

（2）xGBoost加入正则项控制复杂度。

（3）xGBoost对代价函数二阶展开，同时一阶二阶导数

（4）xGBoost支持多种类型的基分类器。GBDT采用CART。

（5）xGBoost自动学习缺失值。

**13　生成式对抗网络**

13.1　初识GANs 的秘密

训练：训练判别器，先固定生成器。

训练生成器，先固定判别器。

对比先训练生成器和先训练判别器区别

值函数：纳什均衡点，MinMax游戏。

训练本质：最小化生成分布和真实数据分布的JS距离。

训练问题：G D优化饱和。

13.2　WGAN：抓住低维的幽灵

原GAN陷阱：坍缩模式。

WGAN：推土机距离，随参数变化连续变化，JS距离不能随参数变化连续变化。

训练：判别器，这里是评分器，为样本打分，越真实分越高。

13.3　DCGAN：当GANs 遇上卷积

用于生成图片：

生成器：使用反卷积

去掉池化层

分数步进卷积：升采样

批量归一化和RELU激活

判别器：使用LRELU内部激活，最后不用全连接

13.4　ALI：包揽推断业务

推断网络：建立数据空间到隐空间的映射。判别网络：区分来自生成网络还是推断网络。

13.5　IRGAN：生成离散样本

13.6　SeqGAN：生成文本序列

构建生成器：序列建模LSTM 优化目标：使用强化学习梯度：策略和动作值函数。

#**14　人工智能的热门应用**

##14.1　计算广告

14.2　游戏中的人工智能

14.3　AI 在自动驾驶中的应用

14.4　机器翻译

14.5　人机交互中的智能计算
 





