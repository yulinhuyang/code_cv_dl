# **1　特征工程**

## 1.1　特征归一化

线性函数（min-max）归一化：0-1

零均值归一化

决策树节点分裂主要依据信息增益比，是否归一化影响不大。

## 1.2　类别型特征

序号编码

独热编码：稀疏向量，配合特征选择来降低维度。

二进制编码

## 1.3　高维组合特征的处理

## 1.4　组合特征

## 1.5　文本表示模型

词袋模型

TF-IDF：单词t在文档中出现到频率  乘以逆文档频率（单词t对表达语义的重要性）

N-gram：n个词组成词组作为一个单独的特征放到向量表中去。

主题模型：主题分布。

词嵌入：词向量化。

## 1.6　Word2Vec

CBOW:.根据上下文词语，预测当前词生成概率。

skip-gram:根据当前词预测上下文各词生成概率。

LDA：隐狄利克雷，利用文档中单词的共现关系进行聚类，概率图模型。

## 1.7　图像数据不足时的处理方法

简化模型

数据扩充

数据增强：变换 增强 加噪声 颜色变换（PCA）

SMOTE：特征提取后，特征空间变换

# **2　模型评估**

## 2.1　评估指标的局限性

精确率和召回率的平衡

PR曲线：横轴精确率，纵轴召回率。

一个点代表某个阈值下，大于阈值的为正样本，小于的为负样本。

绘制：整个PR曲线是通过将阈值从高到低移动生成。

F1 score精确率和召回率的调和平均。

回归模型评价：RMSE，离群点存在问题。MAPE：更鲁棒指标。


## 2.2　ROC 曲线

受试者工作特征曲线

横坐标：假阳FPR 纵坐标：TPR真阳 

绘制：动态的调整截断点（区分正负预测结果的阈值）每个截断点对应一个FPR TPR

简单方法：横 1/N 纵 1/P，零点开始，正样本沿纵轴绘一个刻度，负样本沿横轴一个刻度。

AUC计算：0.5-1之间，越大越好

PR和ROC对比：正负样本分布变化时，ROC形状基本不变，PR剧烈变化。

ROC更稳定反应模型好坏。


## 2.3　余弦距离的应用

两个向量相似度为余弦相似度｛-1，1｝，1-相似度为余弦距离，｛0，2｝

向量维度高时候，可以保持特性

欧式距离体现数值上差异，

余弦距离体现方向差异。

特性：正定性 对称性

三角不等式：不满足

KL：不满足对称性和三角不等式。


## 2.4　A/B 测试的陷阱

在线A/B测试必要性：

离线测试不会考虑线上的延迟 缺失标签等情况。

离线：关注ROC PR

在线：用户点击率 留存时长 PV访问量等。

方法：用户分桶，实验组和对照组。

划分方法：


## 2.5　模型评估的方法

Holdout检验：随机划分

交叉检验：k-fold交叉验证 留一验证

自助法：自助采样法，总数为n，有放回抽取n次。

自助法，n趋于无穷大，有36.8数据未被采样过。


## 2.6　超参数调优

网格搜索：先使用较宽范围和较大步长，逐渐缩小搜索范围。

随机搜索：不再测试上界和下界。

贝叶斯优化：先根据先验分布，假设一个搜索函数，然后使用新采样点测试目标函数，再更新先验分布。探索和利用的平衡。


## 2.7　过拟合与欠拟合

降低过拟合风险：更多训练数据 降低复杂度 正则化模型参数 集成学习

降低欠拟合风险：添加新特征（因子分解机 梯度提升决策树） 增加模型复杂度 减小正则化系数


# **3　经典算法**

## 3.1　支持向量机

分类超平面，仅由支持向量决定。

SHT（超平面分离定理）：对于不相交的凸集，存在一个超平面，将两个凸集分离。两个凸集距离最短两点连线中垂线就是一个将他们分离的超平面。训练误差为零。


## 3.2　逻辑回归
逻辑回归处理分类问题，y离散。

线性回归连续。

自变量x和超参确定情况下，逻辑回归可以看作广义线性模型在y服从二元分布时的特殊情况。

多标签分类：k个二分器。

## 3.3　决策树

启发函数：

ID3最大信息增益计算：离散，分类，缺失值敏感。可剪枝权衡，可多分叉。

C4.5最大信息增益比：连续，分类，可多分叉，可剪枝权衡。

CART 最大基尼指数（Gini）：选择基尼指数最小的特征及其对应的切分点进行分类。连续，分类回归都可以。一个节点两个分支。利用全部数据发现所有结构进行对比。

剪枝方法：

预剪枝：在生成决策树过程中，提前停止树的增长。

停止生长判断：一定深度；当前节点样本量小于某个阈值；分裂对准确度提升小于某个阈值。

后剪枝：在已生成的过拟合决策树上进行剪枝。

方法：REP（错误率降低剪枝）

PEP（悲观剪枝）

CCP（代价复杂度剪枝）

MEP（最小误差剪枝）

CVP   OPP

选择真实误差最小方法：基于独立剪枝；k折交叉


# **4　降维**

## 4.1　PCA 最大方差理论

信噪比：越大越好。

目标：最大化投影方差，让数据在主轴的投影方差最大。

最佳投影：最大特征值对应特征向量。

次最佳投影：第二大特征值对应向量。

KPCA 核主成分分析

## 4.2  PCA 最小平方误差理论

PCA求解实际就是最佳投影方向，即一条直线，最小平方误差 和数学中线性回归问题类似。

## 4.3　线性判别分析

LDA :Fisher LDA 

PCA没有考虑数据标签（类别）

LDA考虑了标签类别，为分类服务，投影后的样本尽可能按照原始类别分开。

中心思想：最大化类间和最小化类内距离。

## 4.4　线性判别分析与主成分分析

PCA：非监督算法，选择投影后数据方差最大的方向。

LDA：有监督降维，选择投影后类内方差小，类间方差大的方向。

总体散度=类内散度+类间散度

PCA人脸识别：特征脸

降维：无监督 PCA 有监督 LDA

# **5　非监督学习**

## 5.1　K均值聚类

思想：通过迭代寻找K个簇的划分

优缺点：结果通常局部最优；大数据集，相对可伸缩和高效。

调优：数据归一化和离群点处理；合理选择K值（手肘法，GAP statistic，GAP（k）随机样本损失和实际样本损失之差）；采样核函数（核K均值算法，通过非线性映射，将输入空间中的数据点映射到高位的特征空间，并在新的特征空间进行聚类）

改进：K-mean++(聚类中心初始选择，距离当前几个中心较远的点)     
ISODATA（迭代自组织数据分析法，思想：某个类别样本数过少则去除该类，某类样本过多分散则分为两类；分离与合并操作）

## 5.2　高斯混合模型

GMM：假设每个簇的数据都符合高斯（正态）分布，当前数据呈现的分布是各个簇高斯分布叠加在一起的结果。

使用EM算法迭代计算。

高斯混合模型：用多个高斯分布函数的线性组合来对数据分布进行拟合。

生成式模型。

指定k，计算最佳的K个高斯分模型

E：根据当前参数，计算每个点由某个分模型生成的概率。

M：使用E步骤估计出的概率，来改进每个分模型的均值方差和权重。

## 5.3　自组织映射神经网络

SOM

训练时：竞争学习，每个输入样例在输出层找到一个和它最匹配的节点。

拓扑关系：一维线阵，二维平面阵，三维栅格阵。

学习过程：初始化，   竞争（计算机输入模式判别函数值，最小的特定神经元为胜利者），   合作（根据获胜神经元，更新拓扑邻域节点），  适应（调整相关神经元连接权重，使得获胜神经元对相似输入模式后续响应增强），迭代。

神经元激励：墨西哥帽，近邻兴奋，远邻抑制。


## 5.4　非监督学习算法的评估

数据簇分类：中心定义，密度定义，连通定义，概念定义。

评估任务：估计聚类趋势（检测数据分布中是否有非随机簇结构，Hopkins统计量）；判定数据簇数（手肘法和gap statistic）；测定聚类质量(主要考察簇紧凑和分离情况，轮廓系数、均方根标准偏差RMSSTD衡量紧凑度、R方（聚类前后平方误差改进程度）、改进的Hubert统计)。

# **6　概率图模型**

## 6.1　概率图模型的联合概率分布

概率图：贝叶斯（有向图） 马尔可夫网（无向图）

马尔可夫网联合概率分布：

最大团：子集中加入任意其他节点都不能构成一个团。

## 6.2　概率图表示

朴素贝叶斯模型：通过预测指定样本属于特定类别的概率P来预测该样本所属类别。

盘式记法

最大熵模型：在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。

## 6.3　生成式模型与判别式模型

生成式模型：对联合概率分布P（x,y,z）建模，在给定观测集合X条件下，通过计算边缘分布来得到对变量集合Y的推断。

判别式模型：直接对条件概率分布P（Y，Z|X）建模，然后消掉无关的变量Z，就得到对于变量集合Y的预测。

概率图模型：朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫、条件随机场、pLSA、LDA

生成式：朴素贝叶斯、贝叶斯网、pLSA、LDA、隐马尔可夫。

判别式：最大熵模型、条件随机场

## 6.4　马尔可夫模型

马尔可夫过程：tn时刻状态xn，仅与其前一个状态xn-1有关

马尔可夫链：时间和状态取值都离散的马尔可夫过程。

隐马尔可夫模型：对含有未知参数（隐状态）马尔可夫链建模的生成模型。参数包括了隐状态的转移概率、隐到观测状态的输出概率、隐状态的取值空间。

隐马尔可夫模型：概率计算问题（已知模型所有参数，前向后向法计算观测序列Y）、预测问题（已经所有参数和观测序列Y，计算最可能的隐状态X，使用动态规划维特比算法）、学习问题（已知Y，求使得观测序列概率最大的参数，使用Baum-Welch）。

中文分词使用隐马尔可夫：分词问题转序列标注问题。

标注偏置：观测序列中的各个状态仅取决于它对应的隐状态，隐马尔可夫模型建模同时考虑了隐状态间的转移概率和隐状态到观测状态的输出概率。

MEMM：最大熵马尔可夫模型，去掉了隐马尔可夫模型观测状态相互独立的假设。

隐马尔可夫模型是一种对隐状态序列和观测序列的联合概率P（x,y）进行建模的生成式模型。

最大熵马尔可夫模型：直接对标注的后验概率P（y|x）

进行建模的判别式模型。局部归一化带来标注偏置问题（隐状态倾向转移到那些后续状态可能更少的状态）。

条件随机场：CRF，在最大熵马尔可夫模型基础上，进行全局归一化，接近偏置问题。


## 6.5　主题模型

PLSA：频率派，估计文章上的主题分布和主题上的词分布。

LDA：贝叶斯派，PLSA的贝叶斯版本，为主题分布和词分布加了狄利特雷先验（多项式分布的共轭先验概率分布，后验概率仍然服从狄利特雷分布）。

对比PLSA（主题分布和词分布看成确定未知常数）和LDA（待估计参数不是固定常数，而是服从一定分布的随机变量，先验为狄利特雷）

确定LDA主题个数：困惑度，取验证集的困惑度极小值对应的主题个数作为超参数。 HDP-LDA：LDA之上加入分层狄利特雷。

主题模型解决冷启动：用户冷启动、物品冷启动和系统冷启动。解决方法：基于内容推荐。

# **7　优化算法**

## 7.1　有监督学习的损失函数

二分类：0-1损失函数 hinge损失 logistic损失 cross entropy损失

回归：平方损失（MSE）绝对损失 Huber损失

## 7.2　机器学习中的优化问题

凸优化：逻辑回归

非凸优化：主成分分析

## 7.3　经典优化算法

直接法：领回归

迭代法：一阶梯度下降，二阶牛顿法（Hessian）。

BFGS

## 7.4　梯度验证

## 7.5　随机梯度下降法

## 7.6　随机梯度下降法的加速

惯性保持和环境感知：

动量（Momentum）方法：惯性保持

AdaGrad方法：历史梯度和 来衡量不同参数的梯度稀疏性。更新频率低的参数有大的更新步幅，更新频率高的参数步幅可以减小。

Adam:集合惯性保持和环境感知。记录梯度一阶矩（过往梯度和当前梯度平均），体现惯性保持。记录二阶矩，梯度平方平均，体现环境感知。一阶和二阶均采样指数衰减平均技术。

## 7.7　L1 正则化与稀疏性

角度1解空间形状：L2正则项约束后的解空间是圆形。

L1正则项约束后解空间是多边形。多边形更容易在尖角处与等高线碰撞出稀疏解。

角度2 ：函数叠加 

角度3：贝叶斯先验，L1 拉普拉斯先验，L2 高斯先验。

高斯先验极值点更平滑，拉普拉斯更尖峰。

## **8　采样**

# 8.1　采样的作用

信息降维

重采样：自助法和刀切法，可以改变样本分布，更适应后续的模型训练和学习。如重采样处理训练样本不均衡问题。

使用采样方法进行随机模拟，对复杂模型进行近似求解或推理。例如隐狄利克雷模型和DBM求解过程，使用Gibbs采样。

# 8.2　均匀分布随机数

编程实现均匀分布随机数：伪随机数

线性同余法：生成离散均匀分布伪随机数。根据当前生成随机数进行变换。

# 8.3　常见的采样方法

逆变换采样：均匀分布U（0,1）产生一个随机数，然后对随机数进行函数（累积分布函数）逆变换。

拒绝采样：接受拒绝采样，参考分布q随机抽取x，均匀分布产生随机数u，比较u和q/mq，则接受x。

重要性采样：用于计算函数f在目标分布p上的积分。

不需要计算函数积分，只想从目标分布p采样出若干样本，则使用重要性采样，先从参考分布q抽取出N个样本｛xi}，然后按照他们对应的重要性权重wi，对样本进行重新采样，最终得到的样本服从目标分布p(x)。

# 8.4　高斯分布的采样

Box-Muller:单个高斯分布累计分布函数不好求逆，求两个独立的高斯分布的联合分布。

过程：产生（0，1）的两个独立的均匀分布随机数u1,u2

;x=sqrt(-2ln(u1))cos2pi u，需要计算三角函数

Marsaglia polar method避免三角函数计算。

拒绝采样法：效率取决于接受概率的大小，参考分布与目标分布越接近，采样效率越高。

高效的拒绝采样法：Ziggurat采用多个阶梯矩阵逼近目标分布。

# 8.5　马尔科夫蒙特卡洛采样法

MCMC：针对待采样的目标分布，构造一个马尔可夫链，使得该马尔可夫链的平稳分布就是目标分布；然后，从任何一个初始状态出发，沿着马尔可夫链进行状态转移，最终得到的状态转移序列会收敛到目标分布。由此可以得到目标分布的一系列样本。

采样法：MH（Metropolis-Hastings）采样法（首先选择一个容易采样的参考条件分布），Gibbs(吉布斯)采样法（每次只对样本的一个维度进行采样和更新）。

# 8.6　贝叶斯网络的采样

信念网络

祖先采样：节点父节点都完成采样，再对该节点采样。

只考虑一部分变量的边缘分布：用祖先采样先对全部随机变量采样，然后忽视那些不需要的变量采样值。

含有观测变量：网络有观测变量，如何采样。只对非观测变量采样，最终得到的样本需要赋一个重要性权值。也叫似然加权采样。


# 8.7　不均衡样本集的重采样

基于数据的方法：SMOTE方法（对少数样本集的样本x的k近邻，随机选取一个样本y，x和y的连线上选取一点作为合成样本）

Boderline-Smote（分类边界点上的少数样本合成）、ADASYN。

欠采样：informed undersampling(Easy Ensemble算法、Balance Cascade算法（级联结构）、NearMiss算法)

基于算法采样：目标函数改变、异常检测、单类学习


## **9　前向神经网络**

# 9.1　多层感知机与布尔函数

1  MLP表示异或逻辑最少需要几个隐含层（二元输入）：异或XOR，两个隐含层。

Z1：X+Y-1，Z2：-X-Y+1，

输出层：Z=-Z1-Z2+1

2 包含n元输入的任意布尔函数：一个隐含层，需要

：单个隐结点可以表示任意合取范式。

析取范式：n元布尔函数的析取范式最多需要2^（n-1）

个不可规约的合取范式。

3 考虑多隐藏层，实现包含n元输入的任意布尔函数最少需要多少个网络节点和网络层：n元异或函数需要包括3（n-1）个节点，二分下，每层节点两两进行异或，最少需要网络层数为2lgN(2为底)

# 9.2　深度神经网络中的激活函数

Sigmod  tanh

relu：优点：不需要计算指数，只需要一个阈值；非饱和性可以有效解决梯度消失问题，提供相对宽的激活边界；单侧抑制提供了网络的稀疏表达能力。

缺点：局限性导致神经元死亡问题，改进LRELU 、PRELU（负轴参数可学习）

# 9.3　多层感知机的反向传播算法

BP：反向传播
，
平方损失：适合输出连续，最后一层不含sigmod或softmax

交叉熵损失：递推各层参数更新的梯度计算公式。适合二分类或多分类。

# 9.4　神经网络训练技巧

参数随机初始化

Dropout：抑制过拟合，类比Bagging方法

BN：每一层输入之前增加归一化处理，引入超参，功能。

# 9.5　深度卷积神经网络

稀疏交互与参数共享（卷积层具有平移等变性）

池化：最大池化（纹理好）和均值池化（背景保留好），

特殊池化：相邻重叠区域池化和金字塔池化。

池化：平移 伸缩 旋转不变性

文本分类：N-gram

# 9.6　深度残差网络

## **10　循环神经网络**

# 10.1　循环神经网络和卷积神经网络

循环神经网络优点：处理文本数据时，很好处理变长且有序的输入序列。

# 10.2　循环神经网络的梯度消失问题

BPTT：基于时间反向传播。

梯度爆炸：梯度裁剪，梯度范式大于给定值，进行等比收缩。

梯度消失：残差

# 10.3　循环神经网络中的激活函数

relu激活循环神经网络，只有W取值在单位矩阵附近才能取得好效果。

# 10.4　长短期记忆网络

LSTM：输入门 输出门 遗忘门   都需要使用sigmod激活函数

# 10.5　Seq2Seq 模型

编码输入和解码输出，将一个输入序列映射为一个作为输出的序列。

编码向量

方法：解码部分：贪心法；集束搜索（保存b个当前最佳选择，解码时进行扩展和排序）；堆叠RNN；增加Dropout；与编码器建立残差连接；注意力机制。

# 10.6　注意力机制

seq2seq:隐状态及上一个输出词决定当前输出词。

隐入注意力机制

使用双向循环神经网络，加注意力机制。

## **11　强化学习**

# 11.1　强化学习基础

强化学习：环境E 机器人A 状态S 动作A 奖励R

MDP：马尔可夫决策过程，马尔可夫+动态规划，决策者周期或连续观察马尔可夫性的随机动态系统，序贯做出决策。 动作 状态 奖励 时间 状态转移（马尔可夫性） 累积收益

核心任务：学习一个从状态空间S到动作空间A的映射，最大化累积收益。学习算法，Q-learning Actor-Critic

马里奥宝藏：初始化-策略评估-策略提升-重复评估提升

# 11.2　视频游戏里的强化学习

深度强化学习：

经验重放

# 11.3　策略梯度

Q-learning需要状态空间求Q最大值，只适用于离散状态空间，对于连续状态空间，最大值Q难求。

策略梯度：可以无差别处理连续和离散状态。

思想：直接用梯度方法，优化总收益函数。不估计Q，利用当前状态直接生成动作a。

# 11.4　探索与利用

探索：帮助智能体不断试验获得反馈。

利用：利用已有的反馈信息选择最好的动作。

平衡利用和探索：greedy算法

## **12　集成学习**

# 12.1　集成学习的种类

boosting:串行，对前一层分错的样本，给予更高的权重。

bagging：并行，随机森林，基分类器相互独立，训练集划分为若干子集。分治。

# 12.2　集成学习的步骤和例子

训练基分类器：比如ID3

合并基分类器：voting（投票） 和stacking（串行）。

Boosting:分类正确样本降低权重，错误样本升高权重或保持不变。

梯度提升决策树：每一棵树学之前所有树结论和的残差。每个残差就是一个加预测值后得真实值的累加值。

# 12.3　基分类器

常用基分类器：决策树 （原因：样本权重合并训练方便，调节层数折中表达泛化能力，样本扰动对决策树影响比较大）

# 12.4　偏差与方差

偏差：训练模型输出和真实模型输出之间偏差，比如因为模型选择不对导致。

方差：模型复杂度对于训练样本数过高导致。比如阶数过高。

Bagging（bootstrap aggregating）:降低了方差，再抽样，然后在每个样本上训练出来的模型取平均。

Boosting:降低了偏差。

# 12.5　梯度提升决策树的基本原理

GBDT：根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器累加结合到现在模型中。

GBDT：通常使用CART。梯度提升训练。

对比梯度下降和提升：都是在一轮迭代中，利用损失函数相对模型的负梯度方向的信息来对当前模型进行更新。梯度下降：模型参数化形式表示，模型更新等价于参数更新。梯度提升：模型不参数化表示，直接定义在函数空间中。大大扩展了可以使用模型种类。

优点：快，表达泛化好，可解释。

缺点：高维稀疏数据或文本分类特征，优势不明显。需要串行，需要局部并行提高速度。


# 12.6　XGBoost与GBDT 的联系和区别

XGBoost:决策树构建阶段加入正则项。

GBDT：基于负梯度构建新树，完成后剪枝。

XGBoost：最大化分裂前后损失函数差值进行决策树构建。遍历所有特征的所有取值。

总结：

（1）xGBoost是GBDT工程实现

（2）xGBoost加入正则项控制复杂度。

（3）xGBoost对代价函数二阶展开，同时一阶二阶导数

（4）xGBoost支持多种类型的基分类器。GBDT采用CART。

（5）xGBoost自动学习缺失值。

## **13　生成式对抗网络**

# 13.1　初识GANs 的秘密

训练：训练判别器，先固定生成器。

训练生成器，先固定判别器。

对比先训练生成器和先训练判别器区别

值函数：纳什均衡点，MinMax游戏。

训练本质：最小化生成分布和真实数据分布的JS距离。

训练问题：G D优化饱和。

# 13.2　WGAN：抓住低维的幽灵

原GAN陷阱：坍缩模式。

WGAN：推土机距离，随参数变化连续变化，JS距离不能随参数变化连续变化。

训练：判别器，这里是评分器，为样本打分，越真实分越高。

# 13.3　DCGAN：当GANs 遇上卷积

用于生成图片：

生成器：使用反卷积

去掉池化层

分数步进卷积：升采样

批量归一化和RELU激活

判别器：使用LRELU内部激活，最后不用全连接

# 13.4　ALI：包揽推断业务

推断网络：建立数据空间到隐空间的映射。判别网络：区分来自生成网络还是推断网络。

# 13.5　IRGAN：生成离散样本

# 13.6　SeqGAN：生成文本序列

构建生成器：序列建模LSTM 优化目标：使用强化学习梯度：策略和动作值函数。

# **14　人工智能的热门应用**

## 14.1　计算广告

# 14.2　游戏中的人工智能

# 14.3　AI 在自动驾驶中的应用

# 14.4　机器翻译

# 14.5　人机交互中的智能计算
 





