# **1　特征工程**

## 1.1　特征归一化

线性函数（min-max）归一化：0-1

零均值归一化

决策树节点分裂主要依据信息增益比，是否归一化影响不大。

## 1.2　类别型特征

序号编码

独热编码：稀疏向量，配合特征选择来降低维度。

二进制编码

## 1.3　高维组合特征的处理

## 1.4　组合特征

## 1.5　文本表示模型

词袋模型

TF-IDF：单词t在文档中出现到频率  乘以逆文档频率（单词t对表达语义的重要性）

N-gram：n个词组成词组作为一个单独的特征放到向量表中去。

主题模型：主题分布。

词嵌入：词向量化。

## 1.6　Word2Vec

CBOW:.根据上下文词语，预测当前词生成概率。

skip-gram:根据当前词预测上下文各词生成概率。

LDA：隐狄利克雷，利用文档中单词的共现关系进行聚类，概率图模型。

## 1.7　图像数据不足时的处理方法

简化模型

数据扩充

数据增强：变换 增强 加噪声 颜色变换（PCA）

SMOTE：特征提取后，特征空间变换

# **2　模型评估**

## 2.1　评估指标的局限性

精确率和召回率的平衡

PR曲线：横轴精确率，纵轴召回率。

一个点代表某个阈值下，大于阈值的为正样本，小于的为负样本。

绘制：整个PR曲线是通过将阈值从高到低移动生成。

F1 score精确率和召回率的调和平均。

回归模型评价：RMSE，离群点存在问题。MAPE：更鲁棒指标。


## 2.2　ROC 曲线

受试者工作特征曲线

横坐标：假阳FPR 纵坐标：TPR真阳 

绘制：动态的调整截断点（区分正负预测结果的阈值）每个截断点对应一个FPR TPR

简单方法：横 1/N 纵 1/P，零点开始，正样本沿纵轴绘一个刻度，负样本沿横轴一个刻度。

AUC计算：0.5-1之间，越大越好

PR和ROC对比：正负样本分布变化时，ROC形状基本不变，PR剧烈变化。

ROC更稳定反应模型好坏。


## 2.3　余弦距离的应用

两个向量相似度为余弦相似度｛-1，1｝，1-相似度为余弦距离，｛0，2｝

向量维度高时候，可以保持特性

欧式距离体现数值上差异，

余弦距离体现方向差异。

特性：正定性 对称性

三角不等式：不满足

KL：不满足对称性和三角不等式。


## 2.4　A/B 测试的陷阱

在线A/B测试必要性：

离线测试不会考虑线上的延迟 缺失标签等情况。

离线：关注ROC PR

在线：用户点击率 留存时长 PV访问量等。

方法：用户分桶，实验组和对照组。

划分方法：


## 2.5　模型评估的方法

Holdout检验：随机划分

交叉检验：k-fold交叉验证 留一验证

自助法：自助采样法，总数为n，有放回抽取n次。

自助法，n趋于无穷大，有36.8数据未被采样过。


## 2.6　超参数调优

网格搜索：先使用较宽范围和较大步长，逐渐缩小搜索范围。

随机搜索：不再测试上界和下界。

贝叶斯优化：先根据先验分布，假设一个搜索函数，然后使用新采样点测试目标函数，再更新先验分布。探索和利用的平衡。


## 2.7　过拟合与欠拟合

降低过拟合风险：更多训练数据 降低复杂度 正则化模型参数 集成学习

降低欠拟合风险：添加新特征（因子分解机 梯度提升决策树） 增加模型复杂度 减小正则化系数


# **3　经典算法**

## 3.1　支持向量机

分类超平面，仅由支持向量决定。

SHT（超平面分离定理）：对于不相交的凸集，存在一个超平面，将两个凸集分离。两个凸集距离最短两点连线中垂线就是一个将他们分离的超平面。训练误差为零。


## 3.2　逻辑回归
逻辑回归处理分类问题，y离散。

线性回归连续。

自变量x和超参确定情况下，逻辑回归可以看作广义线性模型在y服从二元分布时的特殊情况。

多标签分类：k个二分器。

## 3.3　决策树

启发函数：

ID3最大信息增益计算：离散，分类，缺失值敏感。可剪枝权衡，可多分叉。

C4.5最大信息增益比：连续，分类，可多分叉，可剪枝权衡。

CART 最大基尼指数（Gini）：选择基尼指数最小的特征及其对应的切分点进行分类。连续，分类回归都可以。一个节点两个分支。利用全部数据发现所有结构进行对比。

剪枝方法：

预剪枝：在生成决策树过程中，提前停止树的增长。

停止生长判断：一定深度；当前节点样本量小于某个阈值；分裂对准确度提升小于某个阈值。

后剪枝：在已生成的过拟合决策树上进行剪枝。

方法：REP（错误率降低剪枝）

PEP（悲观剪枝）

CCP（代价复杂度剪枝）

MEP（最小误差剪枝）

CVP   OPP

选择真实误差最小方法：基于独立剪枝；k折交叉


# **4　降维**

## 4.1　PCA 最大方差理论

信噪比：越大越好。

目标：最大化投影方差，让数据在主轴的投影方差最大。

最佳投影：最大特征值对应特征向量。

次最佳投影：第二大特征值对应向量。

KPCA 核主成分分析

## 4.2  PCA 最小平方误差理论

PCA求解实际就是最佳投影方向，即一条直线，最小平方误差 和数学中线性回归问题类似。

## 4.3　线性判别分析

LDA :Fisher LDA 

PCA没有考虑数据标签（类别）

LDA考虑了标签类别，为分类服务，投影后的样本尽可能按照原始类别分开。

中心思想：最大化类间和最小化类内距离。

## 4.4　线性判别分析与主成分分析

PCA：非监督算法，选择投影后数据方差最大的方向。

LDA：有监督降维，选择投影后类内方差小，类间方差大的方向。

总体散度=类内散度+类间散度

PCA人脸识别：特征脸

降维：无监督 PCA 有监督 LDA

# **5　非监督学习**

## 5.1　K均值聚类

思想：通过迭代寻找K个簇的划分

优缺点：结果通常局部最优；大数据集，相对可伸缩和高效。

调优：数据归一化和离群点处理；合理选择K值（手肘法，GAP statistic，GAP（k）随机样本损失和实际样本损失之差）；采样核函数（核K均值算法，通过非线性映射，将输入空间中的数据点映射到高位的特征空间，并在新的特征空间进行聚类）

改进：K-mean++(聚类中心初始选择，距离当前几个中心较远的点)     
ISODATA（迭代自组织数据分析法，思想：某个类别样本数过少则去除该类，某类样本过多分散则分为两类；分离与合并操作）

## 5.2　高斯混合模型

GMM：假设每个簇的数据都符合高斯（正态）分布，当前数据呈现的分布是各个簇高斯分布叠加在一起的结果。

使用EM算法迭代计算。

高斯混合模型：用多个高斯分布函数的线性组合来对数据分布进行拟合。

生成式模型。

指定k，计算最佳的K个高斯分模型

E：根据当前参数，计算每个点由某个分模型生成的概率。

M：使用E步骤估计出的概率，来改进每个分模型的均值方差和权重。

## 5.3　自组织映射神经网络

SOM

训练时：竞争学习，每个输入样例在输出层找到一个和它最匹配的节点。

拓扑关系：一维线阵，二维平面阵，三维栅格阵。

学习过程：初始化，   竞争（计算机输入模式判别函数值，最小的特定神经元为胜利者），   合作（根据获胜神经元，更新拓扑邻域节点），  适应（调整相关神经元连接权重，使得获胜神经元对相似输入模式后续响应增强），迭代。

神经元激励：墨西哥帽，近邻兴奋，远邻抑制。


## 5.4　非监督学习算法的评估

数据簇分类：中心定义，密度定义，连通定义，概念定义。

评估任务：估计聚类趋势（检测数据分布中是否有非随机簇结构，Hopkins统计量）；判定数据簇数（手肘法和gap statistic）；测定聚类质量(主要考察簇紧凑和分离情况，轮廓系数、均方根标准偏差RMSSTD衡量紧凑度、R方（聚类前后平方误差改进程度）、改进的Hubert统计)。

# **6　概率图模型**

## 6.1　概率图模型的联合概率分布

## 6.2　概率图表示

## 6.3　生成式模型与判别式模型

## 6.4　马尔可夫模型

## 6.5　主题模型

# **7　优化算法**

## 7.1　有监督学习的损失函数

## 7.2　机器学习中的优化问题

## 7.3　经典优化算法

## 7.4　梯度验证

## 7.5　随机梯度下降法

## 7.6　随机梯度下降法的加速

## 7.7　L1 正则化与稀疏性

**8　采样**

8.1　采样的作用

8.2　均匀分布随机数

8.3　常见的采样方法

8.4　高斯分布的采样

8.5　马尔科夫蒙特卡洛采样法

8.6　贝叶斯网络的采样

8.7　不均衡样本集的重采样

**9　前向神经网络**

9.1　多层感知机与布尔函数

9.2　深度神经网络中的激活函数

9.3　多层感知机的反向传播算法

9.4　神经网络训练技巧

9.5　深度卷积神经网络

9.6　深度残差网络

**10　循环神经网络**

10.1　循环神经网络和卷积神经网络

10.2　循环神经网络的梯度消失问题

10.3　循环神经网络中的激活函数

10.4　长短期记忆网络

10.5　Seq2Seq 模型

10.6　注意力机制

**11　强化学习**

11.1　强化学习基础

11.2　视频游戏里的强化学习

11.3　策略梯度

11.4　探索与利用

**12　集成学习**

12.1　集成学习的种类

12.2　集成学习的步骤和例子

12.3　基分类器

12.4　偏差与方差

12.5　梯度提升决策树的基本原理

GBDT：根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器累加结合到现在模型中。

GBDT：通常使用CART。梯度提升训练。

对比梯度下降和提升：都是在一轮迭代中，利用损失函数相对模型的负梯度方向的信息来对当前模型进行更新。梯度下降：模型参数化形式表示，模型更新等价于参数更新。梯度提升：模型不参数化表示，直接定义在函数空间中。大大扩展了可以使用模型种类。

优点：快，表达泛化好，可解释。

缺点：高维稀疏数据或文本分类特征，优势不明显。需要串行，需要局部并行提高速度。


12.6　XGBoost与GBDT 的联系和区别

XGBoost:决策树构建阶段加入正则项。

GBDT：基于负梯度构建新树，完成后剪枝。

XGBoost：最大化分裂前后损失函数差值进行决策树构建。遍历所有特征的所有取值。

总结：

（1）xGBoost是GBDT工程实现

（2）xGBoost加入正则项控制复杂度。

（3）xGBoost对代价函数二阶展开，同时一阶二阶导数

（4）xGBoost支持多种类型的基分类器。GBDT采用CART。

（5）xGBoost自动学习缺失值。

**13　生成式对抗网络**

13.1　初识GANs 的秘密

训练：训练判别器，先固定生成器。

训练生成器，先固定判别器。

对比先训练生成器和先训练判别器区别

值函数：纳什均衡点，MinMax游戏。

训练本质：最小化生成分布和真实数据分布的JS距离。

训练问题：G D优化饱和。

13.2　WGAN：抓住低维的幽灵

原GAN陷阱：坍缩模式。

WGAN：推土机距离，随参数变化连续变化，JS距离不能随参数变化连续变化。

训练：判别器，这里是评分器，为样本打分，越真实分越高。

13.3　DCGAN：当GANs 遇上卷积

用于生成图片：

生成器：使用反卷积

去掉池化层

分数步进卷积：升采样

批量归一化和RELU激活

判别器：使用LRELU内部激活，最后不用全连接

13.4　ALI：包揽推断业务

推断网络：建立数据空间到隐空间的映射。判别网络：区分来自生成网络还是推断网络。

13.5　IRGAN：生成离散样本

13.6　SeqGAN：生成文本序列

构建生成器：序列建模LSTM 优化目标：使用强化学习梯度：策略和动作值函数。

#**14　人工智能的热门应用**

##14.1　计算广告

14.2　游戏中的人工智能

14.3　AI 在自动驾驶中的应用

14.4　机器翻译

14.5　人机交互中的智能计算
 





