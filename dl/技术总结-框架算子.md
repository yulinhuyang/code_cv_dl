## tricks

### pytorch

https://github.com/Swall0w/torchstat

[PyTorch Tips(FLOPs计算/参数量/计算图可视化/相关性分析)](https://zhuanlan.zhihu.com/p/112319391)

[PyTorch Cookbook（常用代码段整理合集）](https://zhuanlan.zhihu.com/p/59205847)

[pytorch parallel相关](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)

[Pytorch并行计算：nn.parallel.replicate, scatter, gather, parallel_apply](https://www.cnblogs.com/marsggbo/p/11534141.html)

[Pytorch中Tensor与各种图像格式的相互转化](https://cloud.tencent.com/developer/article/1144751)

[当代研究生应当掌握的并行训练方法（单机多卡）](https://zhuanlan.zhihu.com/p/98535650)

[利用Pytorch实现卷积操作](https://zhuanlan.zhihu.com/p/349683405)

[einsum满足你一切需要：深度学习中的爱因斯坦求和约定](https://zhuanlan.zhihu.com/p/44954540)

https://ajcr.net/Basic-guide-to-einsum/

https://github.com/arogozhnikov/einops


[Pytorch中Tensor与各种图像格式的相互转化](https://cloud.tencent.com/developer/article/1144751)

[万字综述，核心开发者全面解读PyTorch内部机制](https://cloud.tencent.com/developer/article/1442507)


[pytorch维度操作](https://zhuanlan.zhihu.com/p/31495102)

[Pytorch模型训练实用教程](https://github.com/TingsongYu/PyTorch_Tutorial)

[PyTorch 学习笔记（六）：PyTorch的十八个损失函数](https://zhuanlan.zhihu.com/p/61379965)

**pytorch C++ 接口使用** 

https://pytorch.org/cppdocs/frontend.html

[pytorch 并行计算](https://www.cnblogs.com/marsggbo/p/11534141.html)

[libtorch 常用api函数示例](https://www.cnblogs.com/yanghailin/p/12901586.html)

### numpy 

["Stanford 231课程：Python Numpy Tutorial教程"](https://xuepro.github.io/2018/05/09/Python-Numpy-Tutorial/)

### tensorRT

[高性能深度学习支持引擎实战——TensorRT](https://zhuanlan.zhihu.com/p/35657027)

[Syencil/tensorRT](https://github.com/Syencil/tensorRT)

[tiny-tensorrt](https://github.com/zerollzeng/tiny-tensorrt)

[如何使用TensorRT对训练好的PyTorch模型进行加速?](https://zhuanlan.zhihu.com/p/88318324)

[wang-xinyu/tensorrtx](https://github.com/wang-xinyu/tensorrtx)

https://github.com/dusty-nv/jetson-inference

[Ubuntu 18.04 TensorRT 7从Pytorch生成onnx模型开始跑通第一个SampleONNXMNIST示例（C++）](https://blog.csdn.net/catscanner/article/details/107877234)

https://github.com/NVIDIA-AI-IOT/torch2trt

https://developer.nvidia.com/zh-cn/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/

### onnx

[pytorch2onnx 转换](https://github.com/open-mmlab/mmsegmentation/blob/feefc6a9dedbef79cd0375b6b507137b4f2b934c/tools/pytorch2onnx.py)

[caffe2\onnxruntime部署](https://pytorch.apachecn.org/docs/1.4/86.html)

[pytorch onnx支持算子列表](https://pytorch.org/docs/stable/onnx.html#supported-operators)


### numpy

[numpy的文件存储 .npy .npz 文件](https://blog.csdn.net/m0_37041325/article/details/78006203)

### caffe

[caffe特殊层：permute\reshape\flatten\slice\concat](https://blog.csdn.net/liu1152239/article/details/81478313)

### 模型转换工具合集 

https://github.com/longcw/pytorch2caffe

https://github.com/MTlab/onnx2caffe

[模型转换、模型压缩、模型加速工具汇总](https://blog.csdn.net/WZZ18191171661/article/details/99700992)


## notes

### numpy:

np.where 使用取索引法

np.expand_dims增加维度

tile函数功能：对整个数组进行复制拼接

    用法：numpy.tile(a, reps)

    其中a为数组，reps为重复的次数

    >>> np.tile(a,2)  


### tensorRT

cmake使用: 

build目录: 
cd build && cmake.. 
make


### pytorch

tensor用 size表示大小，numpy用shape表示大小

torch.nn模块和torch.nn.functional的区别在于，torch.nn模块在计算时底层调用了torch.nn.functional，

但torch.nn模块包括该层参数，还可以应对训练和测试两种网络状态。使用torch.nn.functional时要注意网络状态


转换

	to(device) 

	tensor.to('cuda:{:d}'.format(i))
	
	tensor.cuda()	

转numpy

	feature.to('cpu').detach().numpy()

numpy转tensor 

	torch.norm(torch.from_numpy(features), p=2, dim=1, keepdim=True)
	
DDP:

    torch.distributed.init_process_group(backend="nccl",
                                        init_method='tcp://127.0.0.1:8555',  # distributed training init method
                                        world_size=1,  # number of nodes for distributed training
                                        rank=0)
 
    model = torch.nn.parallel.DistributedDataParallel(model)

