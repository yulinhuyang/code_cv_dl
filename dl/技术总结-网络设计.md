
### tricks:

[cnn结构设计技巧-兼顾速度精度与工程实现](https://zhuanlan.zhihu.com/p/100609339)

[CNN真的需要下采样（上采样）吗?(原创)](https://zhuanlan.zhihu.com/p/94477174)

[语义分割之Large Kernel Matters](https://zhuanlan.zhihu.com/p/38055224)

[感受野计算器](https://fomoro.com/research/article/receptive-field-calculator)

[聊一聊深度学习的activation function](https://zhuanlan.zhihu.com/p/25110450)



### notes:

**cnn结构设计技巧**

宽度（通道数）决定了网络在某一层学到的信息量，另外因为卷积层能重组通道间的信息，这一操作能让有效信息量增大

感受野决定了网络在某一层看到多大范围，一般说来最后一层一定至少要能看到最大的有意义的物体，更大的感受野通常是无害的。感受野必须缓慢增大，这样才能建模不同距离下的空间相关性。

在达到相同感受野的情况下，多层小卷积核的性能一定比大卷积核更好，因为多层小卷积核的非线性更强，而且更有利于特征共享。

分辨率很重要，尽量不要损失分辨率，为了保住分辨率，在使用下采样之前要保证在这一层上有足够的感受野，这个感受野是相对感受野，是指这一个下采样层相对于上一个下采样层的感受野，把两个下采样之间看成一个子网络的话，这个子网络必须得有一定的感受野才能将空间信息编码到下面的网络去，而具体需要多大的相对感受野，只能实验，一般说来，靠近输入层的层空间信息冗余度最高，所以越靠近输入层相对感受野应该越小。同时在靠近输入层的层，这里可以合成一个大卷积核来降低计算量，因为在输入端，每一层卷积的计算量都非常大。另外相对感受野也必须缓慢变换。

前面几层下采样频率高一点，中间层下采样频率降低，并使用不下采样的方法提高深度。

下采样在网络前几层的密度大一些，（这样能尽可能用微弱精度损失换取速度提升） 越往后下采样的密度应该更小，最终能够下采样的最大深度，以该层的感受野以及数据集中最大的有意义物体尺寸决定

先训一个大模型然后裁剪，也许比直接训一个小模型性能好。

能用可分离卷积替代的卷积一定要替代，一般除了第一个卷积，都能替代，替代完后考虑给替代可分离的通道数乘以2，因为可分离卷积的参数和计算量都是线性增长的，这样做依然有速度增益。同样的道理适用于2+1分离卷积。

多尺度处理方法： 跨层连接本质上都是在做多尺度处理，resnet的shortcut，fpn，和unet的encoderdecoder，这些跨层连接都能融合尺度信息

分割篇：分割同时需要大感受野和高分辨率，分割的感受野要和物体大小相当。空洞卷积不能乱用，感受野缓慢增大是必须的，这样才能建模各种距离的空间相关性。aspp模块一般都用在网络很后面，用来建模不同的scale。空洞卷积和下采样都会损失图像细节，都能增大感受野，这两点是相同的，但是空洞卷积能保证分辨率，所以对于landmark可以更多的使用空洞卷积。



