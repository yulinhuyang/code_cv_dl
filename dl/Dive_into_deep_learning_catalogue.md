# 第1章　深度学习简介           

1.1　起源                   

1.2　发展                  

1.3　成功案例                

1.4　特点                  

                    

# 第2章　预备知识             

## 2.1　获取和运行本书的代码           

2.1.1　获取代码并安装运行环境        

2.1.2　更新代码和运行环境          

2.1.3　使用GPU版的MXNet                           

## 2.2　数据操作                

2.2.1　创建NDArray           

2.2.2　运算               

2.2.3　广播机制             

2.2.4　索引               

2.2.5　运算的内存开销          

2.2.6　NDArray和NumPy相互变换    

## 2.3　自动求梯度               

2.3.1　简单例子              

2.3.2　训练模式和预测模式        

2.3.3　对Python控制流求梯度       

## 2.4　查阅文档                

2.4.1　查找模块里的所有函数和类      

2.4.2　查找特定函数和类的使用      

2.4.3　在MXNet网站上查阅        



# 第3章　深度学习基础          

## 3.1　线性回归                

3.1.1　线性回归的基本要素         

3.1.2　线性回归的表示方法         

## 3.2　线性回归的从零开始实现         

3.2.1　生成数据集             

3.2.2　读取数据集            

3.2.3　初始化模型参数          

3.2.4　定义模型             

3.2.5　定义损失函数           

3.2.6　定义优化算法           

3.2.7　训练模型             

## 3.3　线性回归的简洁实现           

3.3.1　生成数据集             

3.3.2　读取数据集            

3.3.3　定义模型             

3.3.4　初始化模型参数          

3.3.5　定义损失函数           

3.3.6　定义优化算法           

3.3.7　训练模型             

## 3.4　softmax回归              

3.4.1　分类问题              

3.4.2　softmax回归模型         

3.4.3　单样本分类的矢量计算表达式    

3.4.4　小批量样本分类的矢量计算表达式              

3.4.5　交叉熵损失函数          

3.4.6　模型预测及评价          

## 3.5　图像分类数据集（Fashion-MNIST）          

3.5.1　获取数据集             

3.5.2　读取小批量            

## 3.6　softmax回归的从零开始实现       

3.6.1　读取数据集             

3.6.2　初始化模型参数          

3.6.3　实现softmax运算          

3.6.4　定义模型             

3.6.5　定义损失函数           

3.6.6　计算分类准确率          

3.6.7　训练模型             

3.6.8　预测               

## 3.7　softmax回归的简洁实现         

3.7.1　读取数据集             

3.7.2　定义和初始化模型         

3.7.3　softmax和交叉熵损失函数      

3.7.4　定义优化算法           

3.7.5　训练模型             

## 3.8　多层感知机               

3.8.1　隐藏层               

3.8.2　激活函数             

3.8.3　多层感知机            

## 3.9　多层感知机的从零开始实现        

3.9.1　读取数据集             

3.9.2　定义模型参数           

3.9.3　定义激活函数           

3.9.4　定义模型             

3.9.5　定义损失函数           

3.9.6　训练模型             

## 3.10　多层感知机的简洁实现         

3.10.1　定义模型             

3.10.2　训练模型             

## 3.11　模型选择、欠拟合和过拟合        

3.11.1　训练误差和泛化误差        

3.11.2　模型选择             

3.11.3　欠拟合和过拟合          

3.11.4　多项式函数拟合实验       

## 3.12　权重衰减               

3.12.1　方法               

3.12.2　高维线性回归实验         

3.12.3　从零开始实现           

3.12.4　简洁实现                 

## 3.13　丢弃法                

3.13.1　方法               

3.13.2　从零开始实现           

3.13.3　简洁实现             

## 3.14　正向传播、反向传播和计算图      

3.14.1　正向传播            

3.14.2　正向传播的计算图         

3.14.3　反向传播             

3.14.4　训练深度学习模型         

## 3.15　数值稳定性和模型初始化       

3.15.1　衰减和爆炸           

3.15.2　随机初始化模型参数       

## 3.16　实战Kaggle比赛：房价预测       

3.16.1　Kaggle比赛            

3.16.2　读取数据集            

3.16.3　预处理数据集           

3.16.4　训练模型             

3.16.5　k 折交叉验证           

3.16.6　模型选择             

3.16.7　预测并在Kaggle提交结果     

# 第4章　深度学习计算         

## 4.1　模型构造              

4.1.1　继承Block类来构造模型      

4.1.2　Sequential类继承自Block类            

4.1.3　构造复杂的模型         

## 4.2　模型参数的访问、初始化和共享      

4.2.1　访问模型参数            

4.2.2　初始化模型参数          

4.2.3　自定义初始化方法         

4.2.4　共享模型参数           

## 4.3　模型参数的延后初始化          

4.3.1　延后初始化             

4.3.2　避免延后初始化          

## 4.4　自定义层                

4.4.1　不含模型参数的自定义层       

4.4.2　含模型参数的自定义层       

## 4.5　读取和存储               

4.5.1　读写NDArray           

4.5.2　读写Gluon模型的参数       

## 4.6　GPU计算               

4.6.1　计算设备              

4.6.2　NDArray的GPU计算       

4.6.3　Gluon的GPU计算         

# 第5章　卷积神经网络          

## 5.1　二维卷积层               

5.1.1　二维互相关运算           

5.1.2　二维卷积层             

5.1.3　图像中物体边缘检测         

5.1.4　通过数据学习核数组         

5.1.5　互相关运算和卷积运算        

5.1.6　特征图和感受野          
                 

## 5.2　填充和步幅               

5.2.1　填充                

5.2.2　步幅               
            

## 5.3　多输入通道和多输出通道         

5.3.1　多输入通道             

5.3.2　多输出通道            

5.3.3　1×1卷积层            

                   

## 5.4　池化层                 

5.4.1　二维最大池化层和平均池化层               

5.4.2　填充和步幅            

5.4.3　多通道              

                                     

## 5.5　卷积神经网络（LeNet）         

5.5.1　LeNet模型             

5.5.2　训练模型             
                          

## 5.6　深度卷积神经网络（AlexNet）      

5.6.1　学习特征表示            

5.6.2　AlexNet             

5.6.3　读取数据集            

5.6.4　训练模型             

                                    

## 5.7　使用重复元素的网络（VGG）      

5.7.1　VGG块              

5.7.2　VGG网络             

5.7.3　训练模型             
         

## 5.8　网络中的网络（NiN）          

5.8.1　NiN块               

5.8.2　NiN模型              

5.8.3　训练模型             

## 5.9　含并行连结的网络（GoogLeNet）    

5.9.1　Inception块            

5.9.2　GoogLeNet模型          

5.9.3　训练模型             
                           

## 5.10　批量归一化              

5.10.1　批量归一化层          

5.10.2　从零开始实现           

5.10.3　使用批量归一化层的LeNet      

5.10.4　简洁实现             
       

## 5.11　残差网络（ResNet）         

5.11.1　残差块              

5.11.2　ResNet模型           

5.11.3　训练模型            

                           

## 5.12　稠密连接网络（DenseNet）      

5.12.1　稠密块              

5.12.2　过渡层              

5.12.3　DenseNet模型          

5.12.4　训练模型             

                                  

# 第6 章　循环神经网络          

## 6.1　语言模型               

6.1.1　语言模型的计算           

6.1.2　n 元语法              
                       

## 6.2　循环神经网络              

6.2.1　不含隐藏状态的神经网络       

6.2.2　含隐藏状态的循环神经网络     

6.2.3　应用：基于字符级循环神经网络的语言模型              

## 6.3　语言模型数据集（歌词）    

6.3.1　读取数据集             

6.3.2　建立字符索引           

6.3.3　时序数据的采样          
    

## 6.4　循环神经网络的从零开始实现       

6.4.1　one-hot向量            

6.4.2　初始化模型参数          

6.4.3　定义模型             

6.4.4　定义预测函数           

6.4.5　裁剪梯度             

6.4.6　困惑度              

6.4.7　定义模型训练函数         

6.4.8　训练模型并创作歌词        

## 6.5　循环神经网络的简洁实现         

6.5.1　定义模型              

6.5.2　训练模型             

## 6.6　通过时间反向传播            

6.6.1　定义模型              

6.6.2　模型计算图            

6.6.3　方法               

## 6.7　门控循环单元（GRU）         

6.7.1　门控循环单元            

6.7.2　读取数据集            

6.7.3　从零开始实现           

6.7.4　简洁实现             


## 6.8　长短期记忆（LSTM）          

6.8.1　长短期记忆             

6.8.2　读取数据集            

6.8.3　从零开始实现           

6.8.4　简洁实现             

## 6.9　深度循环神经网络                                            

## 6.10　双向循环神经网络                          
           

# 第7 章　优化算法            

## 7.1　优化与深度学习             

7.1.1　优化与深度学习的关系        

7.1.2　优化在深度学习中的挑战       
         

## 7.2　梯度下降和随机梯度下降         

7.2.1　一维梯度下降            

7.2.2　学习率              

7.2.3　多维梯度下降           

7.2.4　随机梯度下降           
         

## 7.3　小批量随机梯度下降           

7.3.1　读取数据集             

7.3.2　从零开始实现           

7.3.3　简洁实现             
                

## 7.4　动量法                

7.4.1　梯度下降的问题           

7.4.2　动量法              

7.4.3　从零开始实现           

7.4.4　简洁实现             
                    

## 7.5　AdaGrad算法            

7.5.1　算法                

7.5.2　特点               

7.5.3　从零开始实现           

7.5.4　简洁实现             
         

## 7.6　RMSProp算法            

7.6.1　算法                

7.6.2　从零开始实现           

7.6.3　简洁实现             
                          

## 7.7　AdaDelta算法             

7.7.1　算法               

7.7.2　从零开始实现           

7.7.3　简洁实现             
                            

## 7.8　Adam算法               

7.8.1　算法                

7.8.2　从零开始实现           

7.8.3　简洁实现             
          

# 第8章　计算性能            

## 8.1　命令式和符号式混合编程         

8.1.1　混合式编程取两者之长        

8.1.2　使用HybridSequential类构造模型                

8.1.3　使用HybridBlock类构造模型               

## 8.2　异步计算               

8.2.1　MXNet中的异步计算        

8.2.2　用同步函数让前端等待计算结果                

8.2.3　使用异步计算提升计算性能     

8.2.4　异步计算对内存的影响       

## 8.3　自动并行计算             

8.3.1　CPU和GPU的并行计算       

8.3.2　计算和通信的并行计算                                 

## 8.4　多GPU计算              

8.4.1　数据并行              

8.4.2　定义模型             

8.4.3　多GPU之间同步数据        

8.4.4　单个小批量上的多GPU训练                

8.4.5　定义训练函数           

8.4.6　多GPU训练实验          

## 8.5　多GPU计算的简洁实现         

8.5.1　多GPU上初始化模型参数     

8.5.2　多GPU训练模型          
         

# 第9章　计算机视觉           

## 9.1　图像增广              

9.1.1　常用的图像增广方法         

9.1.2　使用图像增广训练模型        

## 9.2　微调                 

热狗识别                 
          

## 9.3　目标检测和边界框           

边界框                  

## 9.4　锚框                  

9.4.1　生成多个锚框           

9.4.2　交并比              

9.4.3　标注训练集的锚框         

9.4.4　输出预测边界框          
                          

## 9.5　多尺度目标检测            
         

## 9.6　目标检测数据集（皮卡丘）       

9.6.1　获取数据集             

9.6.2　读取数据集            

9.6.3　图示数据             
                  

## 9.7　单发多框检测（SSD）          

9.7.1　定义模型             

9.7.2　训练模型             

9.7.3　预测目标             
    

## 9.8　区域卷积神经网络（R-CNN）系列   

9.8.1　R-CNN              

9.8.2　Fast R-CNN           

9.8.3　Faster R-CNN          

9.8.4　Mask R-CNN           
 

## 9.9　语义分割和数据集           

9.9.1　图像分割和实例分割         

9.9.2　Pascal VOC 2语义分割数据集               
     

## 9.10　全卷积网络（FCN）         

9.10.1　转置卷积层           

9.10.2　构造模型             

9.10.3　初始化转置卷积层        

9.10.4　读取数据集            

9.10.5　训练模型            

9.10.6　预测像素类别          
                   

## 9.11　样式迁移               

9.11.1　方法               

9.11.2　读取内容图像和样式图像     

9.11.3　预处理和后处理图像       

9.11.4　抽取特征            

9.11.5　定义损失函数          

9.11.6　创建和初始化合成图像      

9.11.7　训练模型            
    

## 9.12　实战Kaggle比赛：图像分类（CIFAR-10）         

9.12.1　获取和整理数据集         

9.12.2　图像增广             

9.12.3　读取数据集            

9.12.4　定义模型            

9.12.5　定义训练函数           

9.12.6　训练模型             

9.12.7　对测试集分类并在Kaggle提交结果             

## 9.13　实战Kaggle比赛：狗的品种识别（ImageNet Dogs）       

9.13.1　获取和整理数据集        

9.13.2　图像增广             

9.13.3　读取数据集            

9.13.4　定义模型             

9.13.5　定义训练函数           

9.13.6　训练模型             

9.13.7　对测试集分类并在Kaggle提交结果             
      

# 第10章　自然语言处理         

## 10.1　词嵌入（word2vec）         

10.1.1　为何不采用one-hot向量      

10.1.2　跳字模型             

10.1.3　连续词袋模型           
                          

## 10.2　近似训练              

10.2.1　负采样              

10.2.2　层序softmax           
      

## 10.3　word2vec的实现          

10.3.1　预处理数据集           

10.3.2　负采样              

10.3.3　读取数据集            

10.3.4　跳字模型             

10.3.5　训练模型             

10.3.6　应用词嵌入模型          

## 10.4　子词嵌入（fastText）        
      

## 10.5　全局向量的词嵌入（GloVe）     

10.5.1　GloVe模型           

10.5.2　从条件概率比值理解GloVe模型           

                                     

## 10.6　求近义词和类比词          

10.6.1　使用预训练的词向量       

10.6.2　应用预训练词向量         

                                     

## 10.7　文本情感分类：使用循环神经网络    

10.7.1　文本情感分类数据集       

10.7.2　使用循环神经网络的模型     

                                   

## 10.8　文本情感分类：使用卷积神经网络（textCNN）          

10.8.1　一维卷积层            

10.8.2　时序最大池化层          

10.8.3　读取和预处理IMDb数据集              

10.8.4　textCNN模型           
           

## 10.9　编码器-解码器（seq2seq）     

10.9.1　编码器             

10.9.2　解码器              

10.9.3　训练模型            
           

## 10.10　 束搜索               

10.10.1　贪婪搜索             

10.10.2　穷举搜索            

10.10.3　束搜索             

                              

## 10.11　注意力机制             

10.11.1　计算背景变量           

10.11.2　更新隐藏状态           

10.11.3　发展                       

## 10.12　机器翻译               

10.12.1　读取和预处理数据集       

10.12.2　含注意力机制的编码器-解码器          

10.12.3　训练模型            

10.12.4　预测不定长的序列        

10.12.5　评价翻译结果          

                   

                   

附录A　数学基础            

附录B　使用 Jupyter 记事本       

附录C　使用 AWS 运行代码       

附录D　GPU 购买指南         

附录E　如何为本书做贡献        

附录F　d2lzh 包索引          

附录G　中英文术语对照表        

参考文献               

索引                 


